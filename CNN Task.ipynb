{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import math\n",
    "import os.path\n",
    "from six.moves import xrange\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.cluster import Birch\n",
    "import  glob\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Doc = collections.namedtuple('Doc',['investigator', 'amount', 'abstract', 'p_num', 'p_name'])\n",
    "def load2(fn, i):\n",
    "    \n",
    "    doc = nltk.data.load(fn)\n",
    "    wrds = nltk.tokenize.wordpunct_tokenize(doc)\n",
    "    try:\n",
    "        investigator = \" \".join(wrds[wrds.index(\"Investigator\")+2:\n",
    "                          min(wrds.index('@' if '@' in wrds else \"Abstract\", \n",
    "                                          wrds.index(\"Investigator\"))-1, \n",
    "                              wrds.index('(', wrds.index(\"Investigator\")))])\n",
    "    except ValueError:\n",
    "        return load2(filenames[i+1], i+1)\n",
    "    amount = int(wrds[wrds.index(\"Amt\") + 4: wrds.index('(', wrds.index(\"Amt\"))][0])\n",
    "    pg = wrds[wrds.index('Program', wrds.index('Sponsor'))+2: wrds.index('Fld')]\n",
    "    abstract = \" \".join(wrds[wrds.index('Abstract')+2:])\n",
    "    try:\n",
    "        return Doc(investigator, amount, abstract, pg[0], \" \".join(pg[1:]))\n",
    "    except ValueError:\n",
    "        pg = wrds[wrds.index(':', wrds.index('Program'))+1: wrds.index('Fld', wrds.index('Program'))]\n",
    "        return Doc(investigator, amount, abstract, pg[0], \" \".join(pg[1:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    \n",
    "    def __init__(self, sequence_length, num_classes, vocab_size,\n",
    "                embeddings_size, filter_sizes, num_filters, embeds,\n",
    "                l2_reg_lambda):\n",
    "        \n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "        \n",
    "        l2_loss = tf.constant(0.0)\n",
    "        \n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            W = tf.Variable(tf.constant(embeds), name='W')\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "        \n",
    "        \n",
    "        pooled_outputs = []\n",
    "        \n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                filter_shape = [filter_size, embeddings_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded, W, strides=[1,1,1,1], \n",
    "                    padding=\"VALID\", name=\"conv\")\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h, ksize=[1,sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1,1,1,1], padding=\"VALID\", name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "        \n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.pooled_outputs = pooled_outputs\n",
    "        self.h_pool = tf.concat(3, pooled_outputs),\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "        \n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "        \n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\", shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(.1, shape=[num_classes]), name=\"b\")\n",
    "            \n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            \n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "        \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(self.scores, self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "        \n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(embeddings, dictionary, data, batch_size, seq_len, dropout_keep_prob):\n",
    "    with tf.Graph().as_default():\n",
    "        \n",
    "        session = tf.Session()\n",
    "        with session.as_default():\n",
    "            cnn = TextCNN(\n",
    "            sequence_length = seq_len*2-1,\n",
    "            num_classes=len(unique),\n",
    "            vocab_size = embeddings.shape[0],\n",
    "            embeddings_size = embeddings.shape[1],\n",
    "            filter_sizes=[3,4,5],\n",
    "            num_filters=128, embeds=embeddings,\n",
    "             l2_reg_lambda=.1)\n",
    "\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            optimizer = tf.train.AdamOptimizer(.0001)\n",
    "            grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "\n",
    "            grad_summaries = []\n",
    "            for g, v in grads_and_vars:\n",
    "                if g is not None:\n",
    "                    grad_hist_summary = tf.histogram_summary(\"{}/grad/hist\".format(v.name), g)\n",
    "                    sparsity_summary = tf.scalar_summary(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                    grad_summaries.append(grad_hist_summary)\n",
    "                    grad_summaries.append(sparsity_summary)\n",
    "            grad_summaries_merged = tf.merge_summary(grad_summaries)\n",
    "\n",
    "            # Output directory for models and summaries\n",
    "            timestamp = str(int(time.time()))\n",
    "            out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "            print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "            # Summaries for loss and accuracy\n",
    "            loss_summary = tf.scalar_summary(\"loss\", cnn.loss)\n",
    "            acc_summary = tf.scalar_summary(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "            # Train Summaries\n",
    "            train_summary_op = tf.merge_summary([loss_summary, acc_summary, grad_summaries_merged])\n",
    "            train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "            train_summary_writer = tf.train.SummaryWriter(train_summary_dir, session.graph)\n",
    "\n",
    "            # Dev summaries\n",
    "            dev_summary_op = tf.merge_summary([loss_summary, acc_summary])\n",
    "            dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "            dev_summary_writer = tf.train.SummaryWriter(dev_summary_dir, session.graph)\n",
    "\n",
    "            # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            saver = tf.train.Saver(tf.all_variables())\n",
    "            \n",
    "            session.run(tf.initialize_all_variables())\n",
    "            #saver.restore(session, './runs/1481478375/checkpoints/model-100')\n",
    "\n",
    "            def train_step(x_batch, y_batch, dropout_keep_prob):\n",
    "                feed_dict = {\n",
    "                    cnn.input_x: x_batch,\n",
    "                    cnn.input_y: y_batch,\n",
    "                    cnn.dropout_keep_prob: dropout_keep_prob\n",
    "                }\n",
    "               \n",
    "                _, step, summaries, loss, accuracy = session.run(\n",
    "                    [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "                train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "            \n",
    "            def dev_step(x_batch, y_batch, writer=None):\n",
    "                feed_dict = {\n",
    "                  cnn.input_x: x_batch,\n",
    "                  cnn.input_y: y_batch,\n",
    "                  cnn.dropout_keep_prob: 1.0\n",
    "                }\n",
    "                step, summaries, loss, accuracy = session.run(\n",
    "                    [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "                if writer:\n",
    "                    writer.add_summary(summaries, step)\n",
    "\n",
    "            for i in range(2):\n",
    "                batches = batcher(data, dictionary, batch_size, seq_len)\n",
    "\n",
    "                for batch in batches:\n",
    "                    x_batch, y_batch = batch['x'], batch['y']\n",
    "                    x_batch = complete(dictionary, x_batch)\n",
    "                    train_step(x_batch, y_batch, dropout_keep_prob)\n",
    "                    current_step = tf.train.global_step(session, global_step)\n",
    "                    \n",
    "                    if current_step % 50 == 0:\n",
    "                        print(\"\\nEvaluation:\")\n",
    "                        dev_set = data.sample(frac=.1)\n",
    "                        dev_batch = dev_batcher(dev_set, dictionary, seq_len)\n",
    "                        x_dev, y_dev = dev_batch['x'], dev_batch['y']\n",
    "                        x_dev = complete(dictionary, x_dev)\n",
    "                        dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                        print(\"\")\n",
    "                    if current_step == 50:\n",
    "                        path = saver.save(session, checkpoint_prefix, global_step=current_step)\n",
    "                        print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def complete(dictionary, data):\n",
    "    comp = []\n",
    "    for ab in data:\n",
    "        ab_c = [ab[0]]\n",
    "        for i in range(len(ab)-1):\n",
    "            ab_c.extend([dictionary[(ab[i],ab[i+1])] \n",
    "                        if (ab[i],ab[i+1]) in dictionary \n",
    "                        else dictionary['UNK'], ab[i+1]])\n",
    "        comp += [ab_c]\n",
    "    return comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique = pd.unique(joint.p_num)\n",
    "label_dict = {unique[i]:i for i in range(len(unique))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dev_batcher(data, dictionary, seq_len):\n",
    "    def to_dict(x, seq_len):\n",
    "        bt = []\n",
    "        for ab in x.values:\n",
    "            bt += [[dictionary[wrd]  if wrd in dictionary else dictionary['UNK']\n",
    "                          for wrd in ab.split(\" \")][:seq_len]]\n",
    "        return np.array(bt)\n",
    "                    \n",
    "    matrix = np.zeros((len(data), len(unique)))\n",
    "    for index, p_num in enumerate(data.p_num.values):\n",
    "        matrix[index][label_dict[p_num]] = 1\n",
    "    return {'x': to_dict(data.abstract, seq_len), 'y': matrix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batcher(data, dictionary, batch_size, seq_len):\n",
    "    def to_dict(x, seq_len):\n",
    "        bt = []\n",
    "        for ab in x.values:\n",
    "            bt += [[dictionary[wrd]  if wrd in dictionary else dictionary['UNK']\n",
    "                          for wrd in ab.split(\" \")][:seq_len]]\n",
    "        return np.array(bt)\n",
    "                    \n",
    "    i = 0\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        c = data.iloc[i:i+batch_size]\n",
    "        matrix = np.zeros((batch_size, len(unique)))\n",
    "        for index, p_num in enumerate(c.p_num.values):\n",
    "            \n",
    "            matrix[index][label_dict[p_num]] = 1\n",
    "        yield {'x': to_dict(c.abstract, seq_len), 'y': matrix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree = pickle.load(open('rev_dic.pkl', 'rb'))\n",
    "dictionary = pickle.load(open('dic.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = np.load('./fembd.npy')\n",
    "network = nx.read_gpickle('./network.gpickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filenames = [fn for fn in glob.iglob('./text/Part*/*/*/*.txt', recursive=False)]\n",
    "submission_tups = [load2(filenames[i], i) for i in range(100000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submissions = pd.DataFrame(submission_tups, columns = Doc._fields, )\n",
    "submissions.drop_duplicates(inplace=True)\n",
    "train_groups = submissions.groupby(\"p_name\")\n",
    "\n",
    "joint = train_groups.filter(lambda x: len(x) > 1000)\n",
    "joint = joint.loc[joint.apply(lambda x: len(x['abstract'].split(\" \")) > 120, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = joint.iloc[:12000]\n",
    "test_data = joint.iloc[25000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = train_data.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /home/lenny/Documents/Language/runs/1481480122\n",
      "\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "indices[39,189] = 36849 is not in [0, 36849)\n\t [[Node: embedding/embedding_lookup = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@embedding/W\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](embedding/W/read, _recv_input_x_0)]]\n\nCaused by op 'embedding/embedding_lookup', defined at:\n  File \"/home/lenny/anaconda3/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/lenny/anaconda3/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/lenny/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/lenny/anaconda3/lib/python3.5/site-packages/traitlets/config/application.py\", line 596, in launch_instance\n    app.start()\n  File \"/home/lenny/anaconda3/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/lenny/anaconda3/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/lenny/anaconda3/lib/python3.5/site-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/home/lenny/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lenny/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/lenny/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/lenny/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/lenny/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lenny/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/lenny/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/lenny/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/lenny/anaconda3/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/lenny/anaconda3/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/lenny/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/lenny/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/lenny/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-86-1b98b090152f>\", line 1, in <module>\n    test = train(embeddings[:36849], dictionary, train_data, 50, 120, .6)\n  File \"<ipython-input-84-e8f4c5afe656>\", line 13, in train\n    l2_reg_lambda=.1)\n  File \"<ipython-input-14-f07929f0f6aa>\", line 15, in __init__\n    self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\n  File \"/home/lenny/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/embedding_ops.py\", line 87, in embedding_lookup\n    validate_indices=validate_indices)\n  File \"/home/lenny/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1010, in gather\n    validate_indices=validate_indices, name=name)\n  File \"/home/lenny/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 749, in apply_op\n    op_def=op_def)\n  File \"/home/lenny/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2380, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/lenny/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1298, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): indices[39,189] = 36849 is not in [0, 36849)\n\t [[Node: embedding/embedding_lookup = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@embedding/W\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](embedding/W/read, _recv_input_x_0)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/home/lenny/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lenny/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    953\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lenny/anaconda3/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lenny/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/errors.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    462\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    464\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: indices[39,189] = 36849 is not in [0, 36849)\n\t [[Node: embedding/embedding_lookup = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@embedding/W\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](embedding/W/read, _recv_input_x_0)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-1b98b090152f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m36849\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m120\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-84-e8f4c5afe656>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(embeddings, dictionary, data, batch_size, seq_len, dropout_keep_prob)\u001b[0m\n\u001b[1;32m     92\u001b[0m                     \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                     \u001b[0mx_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomplete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                     \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m                     \u001b[0mcurrent_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-84-e8f4c5afe656>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(x_batch, y_batch, dropout_keep_prob)\u001b[0m\n\u001b[1;32m     66\u001b[0m                 _, step, summaries, loss, accuracy = session.run(\n\u001b[1;32m     67\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_summary_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                     feed_dict)\n\u001b[0m\u001b[1;32m     69\u001b[0m                 \u001b[0mtime_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}: step {}, loss {:g}, acc {:g}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lenny/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lenny/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 915\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    916\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lenny/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 965\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/lenny/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: indices[39,189] = 36849 is not in [0, 36849)\n\t [[Node: embedding/embedding_lookup = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@embedding/W\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](embedding/W/read, _recv_input_x_0)]]\n\nCaused by op 'embedding/embedding_lookup', defined at:\n  File \"/home/lenny/anaconda3/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/lenny/anaconda3/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/lenny/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/lenny/anaconda3/lib/python3.5/site-packages/traitlets/config/application.py\", line 596, in launch_instance\n    app.start()\n  File \"/home/lenny/anaconda3/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/lenny/anaconda3/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/lenny/anaconda3/lib/python3.5/site-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/home/lenny/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lenny/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/lenny/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/lenny/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/lenny/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/lenny/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/lenny/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/lenny/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/lenny/anaconda3/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/lenny/anaconda3/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/lenny/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/lenny/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/lenny/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-86-1b98b090152f>\", line 1, in <module>\n    test = train(embeddings[:36849], dictionary, train_data, 50, 120, .6)\n  File \"<ipython-input-84-e8f4c5afe656>\", line 13, in train\n    l2_reg_lambda=.1)\n  File \"<ipython-input-14-f07929f0f6aa>\", line 15, in __init__\n    self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\n  File \"/home/lenny/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/embedding_ops.py\", line 87, in embedding_lookup\n    validate_indices=validate_indices)\n  File \"/home/lenny/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1010, in gather\n    validate_indices=validate_indices, name=name)\n  File \"/home/lenny/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 749, in apply_op\n    op_def=op_def)\n  File \"/home/lenny/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2380, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/lenny/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1298, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): indices[39,189] = 36849 is not in [0, 36849)\n\t [[Node: embedding/embedding_lookup = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@embedding/W\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](embedding/W/read, _recv_input_x_0)]]\n"
     ]
    }
   ],
   "source": [
    "test = train(embeddings[:36849], dictionary, train_data, 50, 120, .6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
