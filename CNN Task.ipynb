{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import math\n",
    "import os.path\n",
    "from six.moves import xrange\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.cluster import Birch\n",
    "import  glob\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Doc = collections.namedtuple('Doc',['investigator', 'amount', 'abstract', 'p_num', 'p_name'])\n",
    "def load2(fn, i):\n",
    "    \n",
    "    doc = nltk.data.load(fn)\n",
    "    wrds = nltk.tokenize.wordpunct_tokenize(doc)\n",
    "    try:\n",
    "        investigator = \" \".join(wrds[wrds.index(\"Investigator\")+2:\n",
    "                          min(wrds.index('@' if '@' in wrds else \"Abstract\", \n",
    "                                          wrds.index(\"Investigator\"))-1, \n",
    "                              wrds.index('(', wrds.index(\"Investigator\")))])\n",
    "    except ValueError:\n",
    "        return load2(filenames[i+1], i+1)\n",
    "    amount = int(wrds[wrds.index(\"Amt\") + 4: wrds.index('(', wrds.index(\"Amt\"))][0])\n",
    "    pg = wrds[wrds.index('Program', wrds.index('Sponsor'))+2: wrds.index('Fld')]\n",
    "    abstract = \" \".join(wrds[wrds.index('Abstract')+2:])\n",
    "    try:\n",
    "        return Doc(investigator, amount, abstract, pg[0], \" \".join(pg[1:]))\n",
    "    except ValueError:\n",
    "        pg = wrds[wrds.index(':', wrds.index('Program'))+1: wrds.index('Fld', wrds.index('Program'))]\n",
    "        return Doc(investigator, amount, abstract, pg[0], \" \".join(pg[1:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    \n",
    "    def __init__(self, sequence_length, num_classes, vocab_size,\n",
    "                embeddings_size, filter_sizes, num_filters, embeds,\n",
    "                l2_reg_lambda):\n",
    "        \n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "        \n",
    "        l2_loss = tf.constant(0.0)\n",
    "        \n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            W = tf.Variable(tf.constant(embeds), name='W')\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "        \n",
    "        \n",
    "        pooled_outputs = []\n",
    "        \n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                filter_shape = [filter_size, embeddings_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded, W, strides=[1,1,1,1], \n",
    "                    padding=\"VALID\", name=\"conv\")\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h, ksize=[1,sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1,1,1,1], padding=\"VALID\", name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "        \n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.pooled_outputs = pooled_outputs\n",
    "        self.h_pool = tf.concat(3, pooled_outputs),\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "        \n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "        \n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.Variable(\n",
    "                tf.truncated_normal([num_filters_total, num_classes], stddev=.1), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(.1, shape=[num_classes]), name=\"b\")\n",
    "            \n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            \n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "        \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(self.scores, self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses)\n",
    "        \n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(embeddings, dictionary, data, batch_size, seq_len, dropout_keep_prob):\n",
    "    with tf.Graph().as_default():\n",
    "        \n",
    "        session = tf.Session()\n",
    "        with session.as_default():\n",
    "            cnn = TextCNN(\n",
    "            sequence_length = seq_len,\n",
    "            num_classes=len(unique),\n",
    "            vocab_size = embeddings.shape[0],\n",
    "            embeddings_size = embeddings.shape[1],\n",
    "            filter_sizes=[3,4,5],\n",
    "            num_filters=128, embeds=embeddings,\n",
    "             l2_reg_lambda=.2)\n",
    "\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            optimizer = tf.train.AdamOptimizer(.0001)\n",
    "            grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "\n",
    "            grad_summaries = []\n",
    "            for g, v in grads_and_vars:\n",
    "                if g is not None:\n",
    "                    grad_hist_summary = tf.histogram_summary(\"{}/grad/hist\".format(v.name), g)\n",
    "                    sparsity_summary = tf.scalar_summary(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                    grad_summaries.append(grad_hist_summary)\n",
    "                    grad_summaries.append(sparsity_summary)\n",
    "            grad_summaries_merged = tf.merge_summary(grad_summaries)\n",
    "\n",
    "            # Output directory for models and summaries\n",
    "            timestamp = str(int(time.time()))\n",
    "            out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "            print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "            # Summaries for loss and accuracy\n",
    "            loss_summary = tf.scalar_summary(\"loss\", cnn.loss)\n",
    "            acc_summary = tf.scalar_summary(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "            # Train Summaries\n",
    "            train_summary_op = tf.merge_summary([loss_summary, acc_summary, grad_summaries_merged])\n",
    "            train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "            train_summary_writer = tf.train.SummaryWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "            # Dev summaries\n",
    "            dev_summary_op = tf.merge_summary([loss_summary, acc_summary])\n",
    "            dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "            dev_summary_writer = tf.train.SummaryWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "            # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            saver = tf.train.Saver(tf.all_variables())\n",
    "            \n",
    "            #session.run(tf.initialize_all_variables())\n",
    "            saver.restore(session, './runs/1481476533/checkpoints/model-100')\n",
    "\n",
    "            def train_step(x_batch, y_batch, dropout_keep_prob):\n",
    "                feed_dict = {\n",
    "                    cnn.input_x: x_batch,\n",
    "                    cnn.input_y: y_batch,\n",
    "                    cnn.dropout_keep_prob: dropout_keep_prob\n",
    "                }\n",
    "               \n",
    "                _, step, summaries, loss, accuracy = session.run(\n",
    "                    [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "                train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "            \n",
    "            def dev_step(x_batch, y_batch, writer=None):\n",
    "                feed_dict = {\n",
    "                  cnn.input_x: x_batch,\n",
    "                  cnn.input_y: y_batch,\n",
    "                  cnn.dropout_keep_prob: 1.0\n",
    "                }\n",
    "                step, summaries, loss, accuracy = session.run(\n",
    "                    [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "                if writer:\n",
    "                    writer.add_summary(summaries, step)\n",
    "\n",
    "            for i in range(2):\n",
    "                batches = batcher(data, dictionary, batch_size, seq_len)\n",
    "\n",
    "                for batch in batches:\n",
    "                    x_batch, y_batch = batch['x'], batch['y']\n",
    "                    train_step(x_batch, y_batch, dropout_keep_prob)\n",
    "                    current_step = tf.train.global_step(session, global_step)\n",
    "                    \n",
    "                    if current_step % 50 == 0:\n",
    "                        print(\"\\nEvaluation:\")\n",
    "                        dev_set = data.sample(frac=.1)\n",
    "                        dev_batch = dev_batcher(dev_set, dictionary, seq_len)\n",
    "                        x_dev, y_dev = dev_batch['x'], dev_batch['y']\n",
    "                        dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                        print(\"\")\n",
    "                    if current_step == 100:\n",
    "                        path = saver.save(session, checkpoint_prefix, global_step=current_step)\n",
    "                        print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def complete(network, dictionary, data):\n",
    "    data_c = list([data[0]])\n",
    "    for index, clust in enumerate(data[:-1]):\n",
    "            i = 0\n",
    "            ed = KB.get_edge_data(clust, data[index+1])\n",
    "            while ed and i < 5:\n",
    "                data_c.append(ed['number'])\n",
    "                ed = KB.get_edge_data(ed['number'], data[index+1])\n",
    "                i += 1\n",
    "            data_c.append(data[index+1])\n",
    "    return data_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique = pd.unique(joint.p_num)\n",
    "label_dict = {unique[i]:i for i in range(len(unique))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dev_batcher(data, dictionary, seq_len):\n",
    "    def to_dict(x, seq_len):\n",
    "        bt = []\n",
    "        for ab in x.values:\n",
    "            bt += [[dictionary[wrd]  if wrd in dictionary else dictionary['UNK']\n",
    "                          for wrd in ab.split(\" \")][:seq_len]]\n",
    "        return np.array(bt)\n",
    "                    \n",
    "    matrix = np.zeros((len(data), len(unique)))\n",
    "    for index, p_num in enumerate(data.p_num.values):\n",
    "        matrix[index][label_dict[p_num]] = 1\n",
    "    return {'x': to_dict(data.abstract, seq_len), 'y': matrix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batcher(data, dictionary, batch_size, seq_len):\n",
    "    def to_dict(x, seq_len):\n",
    "        bt = []\n",
    "        for ab in x.values:\n",
    "            bt += [[dictionary[wrd]  if wrd in dictionary else dictionary['UNK']\n",
    "                          for wrd in ab.split(\" \")][:seq_len]]\n",
    "        return np.array(bt)\n",
    "                    \n",
    "    i = 0\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        c = data.iloc[i:i+batch_size]\n",
    "        matrix = np.zeros((batch_size, len(unique)))\n",
    "        for index, p_num in enumerate(c.p_num.values):\n",
    "            \n",
    "            matrix[index][label_dict[p_num]] = 1\n",
    "        yield {'x': to_dict(c.abstract, seq_len), 'y': matrix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree = pickle.load(open('rev_dic.pkl', 'rb'))\n",
    "dictionary = pickle.load(open('dic.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = np.load('./fembd.npy')\n",
    "network = nx.read_gpickle('./network.gpickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filenames = [fn for fn in glob.iglob('./text/Part*/*/*/*.txt', recursive=False)]\n",
    "submission_tups = [load2(filenames[i], i) for i in range(100000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submissions = pd.DataFrame(submission_tups, columns = Doc._fields, )\n",
    "submissions.drop_duplicates(inplace=True)\n",
    "train_groups = submissions.groupby(\"p_name\")\n",
    "\n",
    "joint = train_groups.filter(lambda x: len(x) > 1000)\n",
    "joint = joint.loc[joint.apply(lambda x: len(x['abstract'].split(\" \")) > 120, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = joint.iloc[:12000]\n",
    "test_data = joint.iloc[25000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = train_data.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = dev_batcher(train_data.iloc[:100], dictionary, 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /home/lenny/Documents/Language/runs/1481477599\n",
      "\n",
      "2016-12-11T12:33:20.265272: step 101, loss 2.57123, acc 0.2\n",
      "2016-12-11T12:33:20.555859: step 102, loss 2.44694, acc 0.2\n",
      "2016-12-11T12:33:20.837377: step 103, loss 2.47892, acc 0.14\n",
      "2016-12-11T12:33:21.121308: step 104, loss 2.6272, acc 0.16\n",
      "2016-12-11T12:33:21.407936: step 105, loss 2.67007, acc 0.1\n",
      "2016-12-11T12:33:21.692658: step 106, loss 2.64082, acc 0.22\n",
      "2016-12-11T12:33:21.986696: step 107, loss 2.55216, acc 0.22\n",
      "2016-12-11T12:33:22.280616: step 108, loss 2.47494, acc 0.2\n",
      "2016-12-11T12:33:22.565416: step 109, loss 2.45269, acc 0.18\n",
      "2016-12-11T12:33:22.857920: step 110, loss 2.60065, acc 0.14\n",
      "2016-12-11T12:33:23.207120: step 111, loss 2.38928, acc 0.24\n",
      "2016-12-11T12:33:23.480412: step 112, loss 2.67184, acc 0.2\n",
      "2016-12-11T12:33:23.752982: step 113, loss 2.55468, acc 0.2\n",
      "2016-12-11T12:33:24.030355: step 114, loss 2.72852, acc 0.12\n",
      "2016-12-11T12:33:24.313245: step 115, loss 2.49087, acc 0.22\n",
      "2016-12-11T12:33:24.593323: step 116, loss 2.78333, acc 0.06\n",
      "2016-12-11T12:33:24.868242: step 117, loss 2.49323, acc 0.12\n",
      "2016-12-11T12:33:25.170352: step 118, loss 2.5863, acc 0.16\n",
      "2016-12-11T12:33:25.466515: step 119, loss 2.56707, acc 0.18\n",
      "2016-12-11T12:33:25.752564: step 120, loss 2.58311, acc 0.16\n",
      "2016-12-11T12:33:26.044763: step 121, loss 2.42826, acc 0.2\n",
      "2016-12-11T12:33:26.423773: step 122, loss 2.48331, acc 0.24\n",
      "2016-12-11T12:33:26.697615: step 123, loss 2.63199, acc 0.08\n",
      "2016-12-11T12:33:26.967686: step 124, loss 2.54713, acc 0.18\n",
      "2016-12-11T12:33:27.246257: step 125, loss 2.72713, acc 0.08\n",
      "2016-12-11T12:33:27.514080: step 126, loss 2.43668, acc 0.18\n",
      "2016-12-11T12:33:27.793926: step 127, loss 2.70435, acc 0.14\n",
      "2016-12-11T12:33:28.067886: step 128, loss 2.51196, acc 0.08\n",
      "2016-12-11T12:33:28.344941: step 129, loss 2.67162, acc 0.1\n",
      "2016-12-11T12:33:28.664459: step 130, loss 2.30733, acc 0.26\n",
      "2016-12-11T12:33:28.954456: step 131, loss 2.52104, acc 0.2\n",
      "2016-12-11T12:33:29.272360: step 132, loss 2.48611, acc 0.18\n",
      "2016-12-11T12:33:29.640894: step 133, loss 2.72648, acc 0.08\n",
      "2016-12-11T12:33:29.914808: step 134, loss 2.70854, acc 0.12\n",
      "2016-12-11T12:33:30.189027: step 135, loss 2.60878, acc 0.18\n",
      "2016-12-11T12:33:30.463113: step 136, loss 2.50849, acc 0.1\n",
      "2016-12-11T12:33:30.738281: step 137, loss 2.53008, acc 0.18\n",
      "2016-12-11T12:33:31.009246: step 138, loss 2.66621, acc 0.1\n",
      "2016-12-11T12:33:31.282650: step 139, loss 2.53031, acc 0.18\n",
      "2016-12-11T12:33:31.547832: step 140, loss 2.4327, acc 0.2\n",
      "2016-12-11T12:33:31.879736: step 141, loss 2.2412, acc 0.26\n",
      "2016-12-11T12:33:32.174424: step 142, loss 2.67734, acc 0.14\n",
      "2016-12-11T12:33:32.467533: step 143, loss 2.58885, acc 0.2\n",
      "2016-12-11T12:33:32.801123: step 144, loss 2.61078, acc 0.06\n",
      "2016-12-11T12:33:33.141551: step 145, loss 2.52399, acc 0.16\n",
      "2016-12-11T12:33:33.502155: step 146, loss 2.65841, acc 0.14\n",
      "2016-12-11T12:33:33.862465: step 147, loss 2.58792, acc 0.16\n",
      "2016-12-11T12:33:34.137827: step 148, loss 2.34205, acc 0.22\n",
      "2016-12-11T12:33:34.408750: step 149, loss 2.54092, acc 0.24\n",
      "2016-12-11T12:33:34.685249: step 150, loss 2.50665, acc 0.14\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-236-d4a15ea94188>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m31849\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m120\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-235-2b0c4b46eb0c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(embeddings, dictionary, data, batch_size, seq_len, dropout_keep_prob)\u001b[0m\n\u001b[1;32m     98\u001b[0m                         \u001b[0mdev_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                         \u001b[0mx_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdev_batcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                         \u001b[0mdev_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdev_summary_writer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mcurrent_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-235-2b0c4b46eb0c>\u001b[0m in \u001b[0;36mdev_step\u001b[0;34m(x_batch, y_batch, writer)\u001b[0m\n\u001b[1;32m     80\u001b[0m                 step, summaries, loss, accuracy = session.run(\n\u001b[1;32m     81\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_summary_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                     feed_dict)\n\u001b[0m\u001b[1;32m     83\u001b[0m                 \u001b[0mtime_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}: step {}, loss {:g}, acc {:g}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lenny/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lenny/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    886\u001b[0m                 ' to a larger type (e.g. int64).')\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m           \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lenny/anaconda3/lib/python3.5/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \"\"\"\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'y'"
     ]
    }
   ],
   "source": [
    "test = train(embeddings[:31849], dictionary, train_data, 50, 120, .6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31849"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elen(list(filter(lambda key: type(key) == str, list(dictionary.keys()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
