{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import math\n",
    "from six.moves import xrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load(vocab_size):\n",
    "    ppp = nltk.data.load('../../Downloads/ppp.txt', encoding='utf8')\n",
    "    words_p = nltk.tokenize.wordpunct_tokenize(ppp)[130:]\n",
    "    alw = nltk.data.load('../../Downloads/alw.txt', encoding='utf8')\n",
    "    words_a = nltk.tokenize.wordpunct_tokenize(alw)[143:]\n",
    "    words = words_a + words_p\n",
    "    KB = nx.Graph()\n",
    "    KB.add_nodes_from(np.arange(vocab_size))\n",
    "    #return get_sentences(words), KB\n",
    "    return words, KB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn into list of Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_sentences(words):\n",
    "    sentences_word  = []\n",
    "    sent_word = []\n",
    "    for index, word in enumerate(words):\n",
    "        if word in ['?','.','!']:\n",
    "            sent_word += [word]\n",
    "            sentences_word += [sent_word]\n",
    "            sent_word = []\n",
    "        else:\n",
    "            sent_vec += [comb_embed[dictionary[word]]]\n",
    "            sent_word += [word]\n",
    "    return sentences_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Each Batch\n",
    "-- a useful thing would be for batches to be made of few words, a couple sentences, and then be taken from there. idk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "def gen_batch(data, batch_size, skip_window, num_skips):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # target label at the center of the buffer\n",
    "        targets_to_avoid = [ skip_window ]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target]\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    return batch, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn Vocab into Indices\n",
    "-- This was process, but I think this is useful regardless. I will need to change it to accomodate ..... stuff?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(words, dic, revdic, vocab_size):\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocab_size - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dic[word] = len(dic)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dic:\n",
    "            index = dic[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    revdic = dict(zip(dic.values(), dic.keys()))\n",
    "    return data, dic, revdic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Graph Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically we want to make sure that things are being added to the graph. How do we do this. Well. I think we should take them as if they were read in line and then only add things like that. This way we can do all our learning right good without having to stop and add to the graph\n",
    "\n",
    "Adding node likelihood : http://ieeexplore.ieee.org/document/7266560/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_graph(KB, data):\n",
    "    data = np.array(data)\n",
    "    KB.add_edges_from(np.stack([data[:-1], np.roll(data, -1)[:-1]], 1))\n",
    "    return KB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_read = 1000\n",
    "def main():\n",
    "    vocab_size = 5000\n",
    "    words, KB = load(vocab_size)\n",
    "    data, dic, revdic = [],{},{}\n",
    "    \n",
    "    batch_size = 128\n",
    "    embedding_size = 100  # Dimension of the embedding vector.\n",
    "    skip_window = 2      # How many words to consider left and right.\n",
    "    num_skips = 4         # How many times to reuse an input to generate a label.\n",
    "    \n",
    "    # We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "    # validation samples to the words that have a low numeric ID, which by\n",
    "    # construction are also the most frequent.\n",
    "    valid_size = 50    # Random set of words to evaluate similarity on.\n",
    "    valid_window = 200  # Only pick dev samples in the head of the distribution.\n",
    "    valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "    num_sampled = 70    # Number of negative examples to sample.\n",
    "    num_steps = 10000\n",
    "    \n",
    "    num_batches = 1\n",
    "    \n",
    "    KB = \n",
    "#    embeddings = np.random.uniform(-1, 1, (vocab_size, embedding_size))\n",
    "#    for i in range(num_batches):\n",
    "#        data, dic, revdic = build_vocab(words, dic, revdic, vocab_size)\n",
    "#        KB = update_graph(KB, data)\n",
    "#         embeddings = W2V(batch_size, embedding_size, skip_window,\n",
    "#                         num_skips, valid_size, valid_window,\n",
    "#                         valid_examples, num_sampled, len(embeddings),\n",
    "#                         num_steps, revdic, embeddings, data)\n",
    "    return KB\n",
    "\n",
    "KB = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def W2V(batch_size, embedding_size, skip_window, num_skips, valid_size,\n",
    "       valid_window, valid_examples, num_sampled, vocabulary_size,\n",
    "       num_steps, revdic, embeds, data):\n",
    "    graph = tf.Graph()\n",
    "    \n",
    "    with graph.as_default():\n",
    "        train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "        train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "        valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "        \n",
    "        with tf.device('/cpu:0'):\n",
    "            \n",
    "            embeddings = tf.Variable(tf.constant(embeds.astype(np.float32),\n",
    "                                        shape=[vocabulary_size, embedding_size]))\n",
    "            embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "        \n",
    "        nce_weights = tf.Variable(\n",
    "            tf.truncated_normal([vocabulary_size, embedding_size], \n",
    "                               stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "        \n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(nce_weights, nce_biases, embed, train_labels,\n",
    "                          num_sampled, vocabulary_size))\n",
    "        optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "        \n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), \n",
    "                                     1, keep_dims=True))\n",
    "        normalized_embeddings = embeddings / norm\n",
    "        valid_embeddings = tf.nn.embedding_lookup(\n",
    "            normalized_embeddings, valid_dataset)\n",
    "        similarity = tf.matmul(\n",
    "            valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "        \n",
    "        init = tf.initialize_all_variables()\n",
    "    \n",
    "    with tf.Session(graph=graph) as session:\n",
    "        \n",
    "        init.run()\n",
    "        print(\"Initialized\")\n",
    "        #saver = tf.train.Saver({'In'})\n",
    "        \n",
    "        average_loss = 0\n",
    "        for step in xrange(num_steps):\n",
    "            batch_inputs, batch_labels = gen_batch(\n",
    "                data, batch_size, num_skips, skip_window)\n",
    "            feed_dict = {train_inputs : batch_inputs, \n",
    "                         train_labels : batch_labels}\n",
    "            \n",
    "            _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "            average_loss += loss_val\n",
    "\n",
    "        \n",
    "            if step % 2000 == 0:\n",
    "                if step > 0:\n",
    "                    average_loss /= 2000\n",
    "                # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "                print(\"Average loss at step \", step, \": \", average_loss)\n",
    "                average_loss = 0\n",
    "\n",
    "            # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "            if step % 5000 == 0:\n",
    "                sim = similarity.eval()\n",
    "                for i in xrange(valid_size):\n",
    "                    valid_word = revdic[valid_examples[i]]\n",
    "                    top_k = 8 # number of nearest neighbors\n",
    "                    nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                    log_str = \"Nearest to %s:\" % valid_word\n",
    "                    for k in xrange(top_k):\n",
    "                        close_word = revdic[nearest[k]]\n",
    "                        log_str = \"%s %s,\" % (log_str, close_word)\n",
    "                    print(log_str)\n",
    "\n",
    "        final_embeddings = normalized_embeddings.eval()\n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TFKMC(vectors, n_clusters=1000, max_iter=100000):\n",
    "    mbatch = MiniBatchKMeans(n_clusters=n_clusters, batch_size=len(vectors), max_iter=max_iter)\n",
    "    centroids = mbatch.fit(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What To Do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read Gradient Descent Site\n",
    "### 2. Read the Candidate Sampling Thing\n",
    "### 3. Implement KMeans MiniBatch in TensorFlow\n",
    "### 4. Write Graph Adding Code\n",
    "### 5. Write Processing for Transitions into Graph\n",
    "### 6. Implement Word2Vec Using Graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
