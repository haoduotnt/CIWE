{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ppp = nltk.data.load('../Downloads/ppp.txt', encoding='utf8')\n",
    "words_p = nltk.tokenize.word_tokenize(ppp)[122:]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alw = nltk.data.load('../Downloads/alw.txt', encoding='utf8')\n",
    "words_a = nltk.tokenize.word_tokenize(alw)[122:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = words_a + words_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l2 = np.load('./l2_embed3.npy')\n",
    "centroids = np.load('./centroidsl2.npy').item()\n",
    "word_embed = np.load('./embed3.npy')\n",
    "centers = centroids.cluster_centers_\n",
    "c_labels = centroids.labels_ + len(word_embed)\n",
    "c_counts = centroids.counts_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = np.load('./l2embed.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 40], (',', 11834), ('the', 5884), ('.', 5086), ('to', 4942), ('of', 4298), ('and', 4280), ('a', 2623), ('her', 2349), ('in', 2239), ('was', 2186), ('I', 2040), ('she', 1870), ('it', 1737), ('that', 1733), (';', 1732), ('not', 1667), ('you', 1504), ('be', 1414), ('as', 1375)]\n",
      "Sample data [1316, 11, 3, 3704, 2, 8076, 65, 10, 680, 4] ['CHAPTER', 'I', '.', 'Down', 'the', 'Rabbit-Hole', 'Alice', 'was', 'beginning', 'to']\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 10000\n",
    "\n",
    "def build_dataset(words):\n",
    "  count = [['UNK', -1]]\n",
    "  count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "  dictionary = dict()\n",
    "  for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "  data = list()\n",
    "  unk_count = 0\n",
    "  for word in words:\n",
    "    if word in dictionary:\n",
    "      index = dictionary[word]\n",
    "    else:\n",
    "      index = 0  # dictionary['UNK']\n",
    "      unk_count += 1\n",
    "    data.append(index)\n",
    "  count[0][1] = unk_count\n",
    "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "  return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
    "del words_a\n",
    "del words_p # Hint to reduce memory.\n",
    "print('Most common words (+UNK)', count[:20])\n",
    "print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])\n",
    "\n",
    "data_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transition_dict = {}\n",
    "for index, transition in enumerate(l2):\n",
    "    f  = transition[-2].astype(int)\n",
    "    t = transition[-1].astype(int)\n",
    "    if not (f,t) in transition_dict and c_counts[c_labels[index] - len(word_embed)] > 10:\n",
    "        transition_dict[tuple([f,t])] = c_labels[index]\n",
    "        dictionary['l2_' + str(c_labels[index])] = c_labels[index]\n",
    "        reverse_dictionary[c_labels[index]] = 'l2_' + str(c_labels[index])\n",
    "\n",
    "reverse_t_dict = dict(zip(transition_dict.values(), transition_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size += len(centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l2_s_vec = []\n",
    "l2_s_lab = []\n",
    "\n",
    "sentences_vec = []\n",
    "sentences_word  = []\n",
    "sent_word = []\n",
    "sent_vec = []\n",
    "indices = []\n",
    "for index, word in enumerate(filter(lambda word: word in dictionary, words)):\n",
    "    if word in ['?','.','!']:\n",
    "        sent_vec += [embeddings[dictionary[word]]]\n",
    "        sent_word += [word]\n",
    "        if len(sent_vec) >= 3:\n",
    "            sn_w = [sent_word[0]]\n",
    "            sn_v = [sent_vec[0]]\n",
    "            l2_vec = []\n",
    "            l2_lab = []\n",
    "            for index, word in enumerate(sent_word[:-1]):\n",
    "                f = dictionary[word]\n",
    "                t = dictionary[sent_word[index+1]]\n",
    "                pair = (f,t)\n",
    "                if pair in transition_dict:\n",
    "                    sn_v += [embeddings[transition_dict[pair]], sent_vec[index+1]]\n",
    "                    sn_w += ['l2_'+str(transition_dict[pair]), sent_word[index+1]]\n",
    "                    l2_vec += [embeddings[transition_dict[pair]]]\n",
    "                    l2_lab += [transition_dict[pair]]\n",
    "            if len(l2_lab) > 1:\n",
    "                l2_s_vec += [l2_vec]\n",
    "                l2_s_lab += [l2_lab]\n",
    "            sentences_vec += [sn_v]\n",
    "            sentences_word += [sn_w]\n",
    "            l2_vec, l2_lab, sn_v, sn_w = [],[],[],[]\n",
    "        sent_vec = []\n",
    "        sent_word = []\n",
    "    elif word in ['Chapter', 'CHAPTER'] and words[index+1] in ['1','I']:\n",
    "        indices += [len(sentences_vec)]\n",
    "    else:\n",
    "        sent_vec += [embeddings[dictionary[word]]]\n",
    "        sent_word += [word]\n",
    "#del words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences_vec_t = [np.diff(sent, axis=0) for sent in sentences_vec]\n",
    "\n",
    "words_vec_t = np.array([[[float(dictionary[sent[index]]), \n",
    "                         float(dictionary[sent[index+1]])]\n",
    "                        for index in range(len(sent)-1)] for sent in \n",
    "                         sentences_word])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l3_vec_t = [np.diff(sent, axis=0) for sent in l2_s_vec]\n",
    "l3_lab_t = np.array([[[float(sent[index]), \n",
    "                         float(sent[index+1])]\n",
    "                        for index in range(len(sent)-1)] for sent in \n",
    "                         l2_s_lab])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(l3_vec_t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l3 = np.concatenate((l3_vec_t[0], l3_lab_t[0]), axis=1)\n",
    "for index, sent in enumerate(l3_vec_t[1:]):\n",
    "    nsent = np.concatenate((sent, l3_lab_t[index+1]), axis=1)\n",
    "    l3 = np.concatenate((l3, nsent), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LT = np.concatenate((sentences_vec_t[0], words_vec_t[0]), axis=1)\n",
    "for index, sent in enumerate(sentences_vec_t[1:]):\n",
    "    nsent = np.concatenate((sent, words_vec_t[index+1]), axis=1)\n",
    "    l2 = np.concatenate((l2, nsent), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mbatch = MiniBatchKMeans(n_clusters=100, batch_size=3000, max_iter=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1279: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, init_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:630: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, init_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:630: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, init_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:630: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, init_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1328: DeprecationWarning: This function is deprecated. Please call randint(0, 78485 + 1) instead\n",
      "  0, n_samples - 1, self.batch_size)\n"
     ]
    }
   ],
   "source": [
    "centroids = mbatch.fit(l3[:,:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids.counts_.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#centroids = np.load('./centroidsl3.npy').item()\n",
    "\n",
    "test = pd.DataFrame(l3).groupby(centroids.labels_)\n",
    "nearests = test.apply(nearest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_nearest_vector(array, value, index):\n",
    "    indices = np.concatenate((np.arange(index),np.arange(index+1, len(array))))\n",
    "    sq = np.square(array[indices] - value)\n",
    "    idx = sq.sum(axis=1).argmin()\n",
    "    return np.square(array[idx] - value).sum(), idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nearest(cluster):\n",
    "    cluster = cluster.as_matrix()\n",
    "    if len(cluster) < 2:\n",
    "        return \n",
    "    nn1 = []\n",
    "    for index, word in enumerate(cluster):\n",
    "        ret = find_nearest_vector(cluster[:,:-2], cluster[index, :-2], index)\n",
    "        aa,ab = reverse_t_dict[word[-2]]\n",
    "        ac,ad = reverse_t_dict[word[-1]]\n",
    "        ba,bb = reverse_t_dict[cluster[ret[1]][-2]]\n",
    "        bc,bd = reverse_t_dict[cluster[ret[1]][-1]]\n",
    "        the_picks = [reverse_dictionary[word] for word in [aa,ab,ac,ad,ba,bb,bc,bd]]\n",
    "        nn1 += [[ret[0], *the_picks]]\n",
    "    return pd.DataFrame(nn1, columns=['dist','f1f','f1t','f2f','f2t','t1f','t1t','t2f','t2t'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "no_dup_near = nearests.reset_index().drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>level_1</th>\n",
       "      <th>dist</th>\n",
       "      <th>f1f</th>\n",
       "      <th>f1t</th>\n",
       "      <th>f2f</th>\n",
       "      <th>f2t</th>\n",
       "      <th>t1f</th>\n",
       "      <th>t1t</th>\n",
       "      <th>t2f</th>\n",
       "      <th>t2t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57231</th>\n",
       "      <td>71</td>\n",
       "      <td>591</td>\n",
       "      <td>0.004978</td>\n",
       "      <td>towards</td>\n",
       "      <td>Bingley</td>\n",
       "      <td>Bingley</td>\n",
       "      <td>,</td>\n",
       "      <td>Mr.</td>\n",
       "      <td>Bingley</td>\n",
       "      <td>Bingley</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22993</th>\n",
       "      <td>19</td>\n",
       "      <td>945</td>\n",
       "      <td>0.007881</td>\n",
       "      <td>I</td>\n",
       "      <td>Office</td>\n",
       "      <td>inspired</td>\n",
       "      <td>think</td>\n",
       "      <td>I</td>\n",
       "      <td>Office</td>\n",
       "      <td>to</td>\n",
       "      <td>think</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22384</th>\n",
       "      <td>19</td>\n",
       "      <td>336</td>\n",
       "      <td>0.007881</td>\n",
       "      <td>have</td>\n",
       "      <td>require</td>\n",
       "      <td>to</td>\n",
       "      <td>think</td>\n",
       "      <td>have</td>\n",
       "      <td>require</td>\n",
       "      <td>inspired</td>\n",
       "      <td>think</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50534</th>\n",
       "      <td>57</td>\n",
       "      <td>94</td>\n",
       "      <td>0.007881</td>\n",
       "      <td>inspired</td>\n",
       "      <td>think</td>\n",
       "      <td>think</td>\n",
       "      <td>of</td>\n",
       "      <td>to</td>\n",
       "      <td>think</td>\n",
       "      <td>think</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12506</th>\n",
       "      <td>8</td>\n",
       "      <td>5527</td>\n",
       "      <td>0.008085</td>\n",
       "      <td>;</td>\n",
       "      <td>we</td>\n",
       "      <td>To</td>\n",
       "      <td>must</td>\n",
       "      <td>,</td>\n",
       "      <td>we</td>\n",
       "      <td>To</td>\n",
       "      <td>must</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34744</th>\n",
       "      <td>37</td>\n",
       "      <td>333</td>\n",
       "      <td>0.010674</td>\n",
       "      <td>to</td>\n",
       "      <td>his</td>\n",
       "      <td>aunt</td>\n",
       "      <td>,</td>\n",
       "      <td>to</td>\n",
       "      <td>his</td>\n",
       "      <td>aunt</td>\n",
       "      <td>look</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52057</th>\n",
       "      <td>61</td>\n",
       "      <td>316</td>\n",
       "      <td>0.011650</td>\n",
       "      <td>‘I</td>\n",
       "      <td>hope</td>\n",
       "      <td>unforgiving</td>\n",
       "      <td>,</td>\n",
       "      <td>I</td>\n",
       "      <td>hope</td>\n",
       "      <td>unforgiving</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13360</th>\n",
       "      <td>8</td>\n",
       "      <td>6381</td>\n",
       "      <td>0.015838</td>\n",
       "      <td>the</td>\n",
       "      <td>knocking</td>\n",
       "      <td>letter</td>\n",
       "      <td>;</td>\n",
       "      <td>the</td>\n",
       "      <td>knocking</td>\n",
       "      <td>letter</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60357</th>\n",
       "      <td>75</td>\n",
       "      <td>180</td>\n",
       "      <td>0.016458</td>\n",
       "      <td>strength</td>\n",
       "      <td>here</td>\n",
       "      <td>lovers</td>\n",
       "      <td>that</td>\n",
       "      <td>educated</td>\n",
       "      <td>grave</td>\n",
       "      <td>lovers</td>\n",
       "      <td>that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60527</th>\n",
       "      <td>76</td>\n",
       "      <td>6</td>\n",
       "      <td>0.018422</td>\n",
       "      <td>general</td>\n",
       "      <td>know</td>\n",
       "      <td>indeed</td>\n",
       "      <td>what</td>\n",
       "      <td>you</td>\n",
       "      <td>know</td>\n",
       "      <td>indeed</td>\n",
       "      <td>what</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63380</th>\n",
       "      <td>79</td>\n",
       "      <td>1456</td>\n",
       "      <td>0.018422</td>\n",
       "      <td>you</td>\n",
       "      <td>know</td>\n",
       "      <td>Hunsford</td>\n",
       "      <td>my</td>\n",
       "      <td>general</td>\n",
       "      <td>know</td>\n",
       "      <td>Hunsford</td>\n",
       "      <td>my</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58196</th>\n",
       "      <td>71</td>\n",
       "      <td>1556</td>\n",
       "      <td>0.018422</td>\n",
       "      <td>general</td>\n",
       "      <td>know</td>\n",
       "      <td>know</td>\n",
       "      <td>,</td>\n",
       "      <td>you</td>\n",
       "      <td>know</td>\n",
       "      <td>know</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47312</th>\n",
       "      <td>50</td>\n",
       "      <td>2963</td>\n",
       "      <td>0.020740</td>\n",
       "      <td>have</td>\n",
       "      <td>require</td>\n",
       "      <td>find</td>\n",
       "      <td>chief</td>\n",
       "      <td>have</td>\n",
       "      <td>require</td>\n",
       "      <td>strength</td>\n",
       "      <td>here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11737</th>\n",
       "      <td>8</td>\n",
       "      <td>4758</td>\n",
       "      <td>0.024795</td>\n",
       "      <td>Lydia’s</td>\n",
       "      <td>day</td>\n",
       "      <td>before</td>\n",
       "      <td>had</td>\n",
       "      <td>the</td>\n",
       "      <td>day</td>\n",
       "      <td>before</td>\n",
       "      <td>had</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68602</th>\n",
       "      <td>86</td>\n",
       "      <td>1865</td>\n",
       "      <td>0.024925</td>\n",
       "      <td>for</td>\n",
       "      <td>us</td>\n",
       "      <td>educated</td>\n",
       "      <td>grave</td>\n",
       "      <td>for</td>\n",
       "      <td>us</td>\n",
       "      <td>considered</td>\n",
       "      <td>widow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13603</th>\n",
       "      <td>8</td>\n",
       "      <td>6624</td>\n",
       "      <td>0.024925</td>\n",
       "      <td>the</td>\n",
       "      <td>knocking</td>\n",
       "      <td>educated</td>\n",
       "      <td>grave</td>\n",
       "      <td>the</td>\n",
       "      <td>knocking</td>\n",
       "      <td>considered</td>\n",
       "      <td>widow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63447</th>\n",
       "      <td>79</td>\n",
       "      <td>1523</td>\n",
       "      <td>0.025590</td>\n",
       "      <td>tail</td>\n",
       "      <td>inferiority</td>\n",
       "      <td>like</td>\n",
       "      <td>!</td>\n",
       "      <td>less</td>\n",
       "      <td>taken</td>\n",
       "      <td>like</td>\n",
       "      <td>!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30963</th>\n",
       "      <td>32</td>\n",
       "      <td>224</td>\n",
       "      <td>0.025935</td>\n",
       "      <td>strength</td>\n",
       "      <td>here</td>\n",
       "      <td>share</td>\n",
       "      <td>on</td>\n",
       "      <td>considered</td>\n",
       "      <td>widow</td>\n",
       "      <td>share</td>\n",
       "      <td>on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12165</th>\n",
       "      <td>8</td>\n",
       "      <td>5186</td>\n",
       "      <td>0.025935</td>\n",
       "      <td>considered</td>\n",
       "      <td>widow</td>\n",
       "      <td>strength</td>\n",
       "      <td>here</td>\n",
       "      <td>of</td>\n",
       "      <td>the</td>\n",
       "      <td>of</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39589</th>\n",
       "      <td>44</td>\n",
       "      <td>834</td>\n",
       "      <td>0.026562</td>\n",
       "      <td>in</td>\n",
       "      <td>disposed</td>\n",
       "      <td>less</td>\n",
       "      <td>taken</td>\n",
       "      <td>in</td>\n",
       "      <td>disposed</td>\n",
       "      <td>educated</td>\n",
       "      <td>grave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20637</th>\n",
       "      <td>16</td>\n",
       "      <td>980</td>\n",
       "      <td>0.026562</td>\n",
       "      <td>educated</td>\n",
       "      <td>grave</td>\n",
       "      <td>age</td>\n",
       "      <td>of</td>\n",
       "      <td>less</td>\n",
       "      <td>taken</td>\n",
       "      <td>age</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37131</th>\n",
       "      <td>40</td>\n",
       "      <td>1069</td>\n",
       "      <td>0.027818</td>\n",
       "      <td>considered</td>\n",
       "      <td>widow</td>\n",
       "      <td>to-morrow</td>\n",
       "      <td>,</td>\n",
       "      <td>tail</td>\n",
       "      <td>inferiority</td>\n",
       "      <td>to-morrow</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39409</th>\n",
       "      <td>44</td>\n",
       "      <td>654</td>\n",
       "      <td>0.027818</td>\n",
       "      <td>with</td>\n",
       "      <td>life</td>\n",
       "      <td>considered</td>\n",
       "      <td>widow</td>\n",
       "      <td>with</td>\n",
       "      <td>life</td>\n",
       "      <td>tail</td>\n",
       "      <td>inferiority</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13353</th>\n",
       "      <td>8</td>\n",
       "      <td>6374</td>\n",
       "      <td>0.027818</td>\n",
       "      <td>a</td>\n",
       "      <td>pollution</td>\n",
       "      <td>considered</td>\n",
       "      <td>widow</td>\n",
       "      <td>a</td>\n",
       "      <td>pollution</td>\n",
       "      <td>tail</td>\n",
       "      <td>inferiority</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74864</th>\n",
       "      <td>94</td>\n",
       "      <td>401</td>\n",
       "      <td>0.027818</td>\n",
       "      <td>considered</td>\n",
       "      <td>widow</td>\n",
       "      <td>States</td>\n",
       "      <td>;</td>\n",
       "      <td>tail</td>\n",
       "      <td>inferiority</td>\n",
       "      <td>States</td>\n",
       "      <td>;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16952</th>\n",
       "      <td>12</td>\n",
       "      <td>947</td>\n",
       "      <td>0.028088</td>\n",
       "      <td>strength</td>\n",
       "      <td>here</td>\n",
       "      <td>gentlemen</td>\n",
       "      <td>,</td>\n",
       "      <td>less</td>\n",
       "      <td>taken</td>\n",
       "      <td>gentlemen</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14470</th>\n",
       "      <td>8</td>\n",
       "      <td>7491</td>\n",
       "      <td>0.030324</td>\n",
       "      <td>sister</td>\n",
       "      <td>,</td>\n",
       "      <td>in</td>\n",
       "      <td>disposed</td>\n",
       "      <td>sister</td>\n",
       "      <td>that</td>\n",
       "      <td>in</td>\n",
       "      <td>disposed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13545</th>\n",
       "      <td>8</td>\n",
       "      <td>6566</td>\n",
       "      <td>0.030324</td>\n",
       "      <td>dear</td>\n",
       "      <td>work</td>\n",
       "      <td>sister</td>\n",
       "      <td>that</td>\n",
       "      <td>dear</td>\n",
       "      <td>work</td>\n",
       "      <td>sister</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27757</th>\n",
       "      <td>26</td>\n",
       "      <td>309</td>\n",
       "      <td>0.032889</td>\n",
       "      <td>it</td>\n",
       "      <td>as</td>\n",
       "      <td>as</td>\n",
       "      <td>soon</td>\n",
       "      <td>soon</td>\n",
       "      <td>as</td>\n",
       "      <td>as</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11505</th>\n",
       "      <td>8</td>\n",
       "      <td>4526</td>\n",
       "      <td>0.035479</td>\n",
       "      <td>their</td>\n",
       "      <td>defect</td>\n",
       "      <td>concerns</td>\n",
       "      <td>Mr.</td>\n",
       "      <td>their</td>\n",
       "      <td>table</td>\n",
       "      <td>concerns</td>\n",
       "      <td>Mr.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68035</th>\n",
       "      <td>86</td>\n",
       "      <td>1298</td>\n",
       "      <td>0.050440</td>\n",
       "      <td>which</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>simpleton</td>\n",
       "      <td>which</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>fashion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62314</th>\n",
       "      <td>79</td>\n",
       "      <td>390</td>\n",
       "      <td>0.050441</td>\n",
       "      <td>a</td>\n",
       "      <td>subdued</td>\n",
       "      <td>degree</td>\n",
       "      <td>?</td>\n",
       "      <td>a</td>\n",
       "      <td>subdued</td>\n",
       "      <td>aunt</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50139</th>\n",
       "      <td>56</td>\n",
       "      <td>465</td>\n",
       "      <td>0.050441</td>\n",
       "      <td>made</td>\n",
       "      <td>not</td>\n",
       "      <td>aunt</td>\n",
       "      <td>?</td>\n",
       "      <td>made</td>\n",
       "      <td>not</td>\n",
       "      <td>degree</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63154</th>\n",
       "      <td>79</td>\n",
       "      <td>1230</td>\n",
       "      <td>0.050441</td>\n",
       "      <td>in</td>\n",
       "      <td>disposed</td>\n",
       "      <td>degree</td>\n",
       "      <td>?</td>\n",
       "      <td>in</td>\n",
       "      <td>disposed</td>\n",
       "      <td>aunt</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62430</th>\n",
       "      <td>79</td>\n",
       "      <td>506</td>\n",
       "      <td>0.050441</td>\n",
       "      <td>all</td>\n",
       "      <td>too</td>\n",
       "      <td>degree</td>\n",
       "      <td>?</td>\n",
       "      <td>all</td>\n",
       "      <td>too</td>\n",
       "      <td>aunt</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62625</th>\n",
       "      <td>79</td>\n",
       "      <td>701</td>\n",
       "      <td>0.050441</td>\n",
       "      <td>of</td>\n",
       "      <td>advised</td>\n",
       "      <td>degree</td>\n",
       "      <td>?</td>\n",
       "      <td>of</td>\n",
       "      <td>advised</td>\n",
       "      <td>aunt</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63170</th>\n",
       "      <td>79</td>\n",
       "      <td>1246</td>\n",
       "      <td>0.050441</td>\n",
       "      <td>he</td>\n",
       "      <td>fear</td>\n",
       "      <td>degree</td>\n",
       "      <td>?</td>\n",
       "      <td>he</td>\n",
       "      <td>fear</td>\n",
       "      <td>aunt</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63431</th>\n",
       "      <td>79</td>\n",
       "      <td>1507</td>\n",
       "      <td>0.050441</td>\n",
       "      <td>be</td>\n",
       "      <td>library.”</td>\n",
       "      <td>aunt</td>\n",
       "      <td>?</td>\n",
       "      <td>be</td>\n",
       "      <td>library.”</td>\n",
       "      <td>degree</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28575</th>\n",
       "      <td>28</td>\n",
       "      <td>169</td>\n",
       "      <td>0.050441</td>\n",
       "      <td>rustled</td>\n",
       "      <td>you</td>\n",
       "      <td>aunt</td>\n",
       "      <td>?</td>\n",
       "      <td>rustled</td>\n",
       "      <td>you</td>\n",
       "      <td>degree</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56216</th>\n",
       "      <td>69</td>\n",
       "      <td>753</td>\n",
       "      <td>0.050441</td>\n",
       "      <td>to</td>\n",
       "      <td>be</td>\n",
       "      <td>degree</td>\n",
       "      <td>?</td>\n",
       "      <td>to</td>\n",
       "      <td>be</td>\n",
       "      <td>aunt</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63069</th>\n",
       "      <td>79</td>\n",
       "      <td>1145</td>\n",
       "      <td>0.050441</td>\n",
       "      <td>his</td>\n",
       "      <td>only</td>\n",
       "      <td>aunt</td>\n",
       "      <td>?</td>\n",
       "      <td>his</td>\n",
       "      <td>only</td>\n",
       "      <td>degree</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31559</th>\n",
       "      <td>33</td>\n",
       "      <td>551</td>\n",
       "      <td>0.050441</td>\n",
       "      <td>air</td>\n",
       "      <td>to</td>\n",
       "      <td>degree</td>\n",
       "      <td>?</td>\n",
       "      <td>air</td>\n",
       "      <td>to</td>\n",
       "      <td>aunt</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56120</th>\n",
       "      <td>69</td>\n",
       "      <td>657</td>\n",
       "      <td>0.050441</td>\n",
       "      <td>ill-humour</td>\n",
       "      <td>be</td>\n",
       "      <td>aunt</td>\n",
       "      <td>?</td>\n",
       "      <td>ill-humour</td>\n",
       "      <td>be</td>\n",
       "      <td>degree</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7244</th>\n",
       "      <td>8</td>\n",
       "      <td>265</td>\n",
       "      <td>0.050564</td>\n",
       "      <td>,</td>\n",
       "      <td>man.”</td>\n",
       "      <td>upon</td>\n",
       "      <td>way</td>\n",
       "      <td>,</td>\n",
       "      <td>man.”</td>\n",
       "      <td>the</td>\n",
       "      <td>way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44770</th>\n",
       "      <td>50</td>\n",
       "      <td>421</td>\n",
       "      <td>0.050564</td>\n",
       "      <td>the</td>\n",
       "      <td>way</td>\n",
       "      <td>therefore</td>\n",
       "      <td>--</td>\n",
       "      <td>upon</td>\n",
       "      <td>way</td>\n",
       "      <td>therefore</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4783</th>\n",
       "      <td>6</td>\n",
       "      <td>223</td>\n",
       "      <td>0.050564</td>\n",
       "      <td>in</td>\n",
       "      <td>the</td>\n",
       "      <td>upon</td>\n",
       "      <td>way</td>\n",
       "      <td>in</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40839</th>\n",
       "      <td>44</td>\n",
       "      <td>2084</td>\n",
       "      <td>0.051088</td>\n",
       "      <td>his</td>\n",
       "      <td>only</td>\n",
       "      <td>family</td>\n",
       "      <td>Lady</td>\n",
       "      <td>his</td>\n",
       "      <td>only</td>\n",
       "      <td>family</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18277</th>\n",
       "      <td>13</td>\n",
       "      <td>891</td>\n",
       "      <td>0.052798</td>\n",
       "      <td>a</td>\n",
       "      <td>year.”</td>\n",
       "      <td>hoped</td>\n",
       "      <td>.</td>\n",
       "      <td>a</td>\n",
       "      <td>subdued</td>\n",
       "      <td>hoped</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74881</th>\n",
       "      <td>94</td>\n",
       "      <td>418</td>\n",
       "      <td>0.052798</td>\n",
       "      <td>a</td>\n",
       "      <td>subdued</td>\n",
       "      <td>cost</td>\n",
       "      <td>;</td>\n",
       "      <td>a</td>\n",
       "      <td>year.”</td>\n",
       "      <td>cost</td>\n",
       "      <td>;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69569</th>\n",
       "      <td>87</td>\n",
       "      <td>644</td>\n",
       "      <td>0.052798</td>\n",
       "      <td>a</td>\n",
       "      <td>subdued</td>\n",
       "      <td>of</td>\n",
       "      <td>the</td>\n",
       "      <td>a</td>\n",
       "      <td>year.”</td>\n",
       "      <td>of</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67000</th>\n",
       "      <td>86</td>\n",
       "      <td>263</td>\n",
       "      <td>0.052798</td>\n",
       "      <td>executors</td>\n",
       "      <td>maternal</td>\n",
       "      <td>a</td>\n",
       "      <td>subdued</td>\n",
       "      <td>executors</td>\n",
       "      <td>maternal</td>\n",
       "      <td>a</td>\n",
       "      <td>year.”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66482</th>\n",
       "      <td>85</td>\n",
       "      <td>604</td>\n",
       "      <td>0.052798</td>\n",
       "      <td>a</td>\n",
       "      <td>subdued</td>\n",
       "      <td>courtship</td>\n",
       "      <td>.</td>\n",
       "      <td>a</td>\n",
       "      <td>year.”</td>\n",
       "      <td>courtship</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74147</th>\n",
       "      <td>93</td>\n",
       "      <td>767</td>\n",
       "      <td>0.052798</td>\n",
       "      <td>,</td>\n",
       "      <td>after</td>\n",
       "      <td>a</td>\n",
       "      <td>subdued</td>\n",
       "      <td>,</td>\n",
       "      <td>after</td>\n",
       "      <td>a</td>\n",
       "      <td>year.”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49643</th>\n",
       "      <td>55</td>\n",
       "      <td>949</td>\n",
       "      <td>0.052798</td>\n",
       "      <td>a</td>\n",
       "      <td>year.”</td>\n",
       "      <td>,</td>\n",
       "      <td>and</td>\n",
       "      <td>a</td>\n",
       "      <td>subdued</td>\n",
       "      <td>,</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18022</th>\n",
       "      <td>13</td>\n",
       "      <td>636</td>\n",
       "      <td>0.052798</td>\n",
       "      <td>a</td>\n",
       "      <td>subdued</td>\n",
       "      <td>removed</td>\n",
       "      <td>.</td>\n",
       "      <td>a</td>\n",
       "      <td>year.”</td>\n",
       "      <td>removed</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68267</th>\n",
       "      <td>86</td>\n",
       "      <td>1530</td>\n",
       "      <td>0.052798</td>\n",
       "      <td>a</td>\n",
       "      <td>year.”</td>\n",
       "      <td>across</td>\n",
       "      <td>before</td>\n",
       "      <td>a</td>\n",
       "      <td>subdued</td>\n",
       "      <td>across</td>\n",
       "      <td>before</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74305</th>\n",
       "      <td>93</td>\n",
       "      <td>925</td>\n",
       "      <td>0.052798</td>\n",
       "      <td>is</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>subdued</td>\n",
       "      <td>is</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>year.”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60325</th>\n",
       "      <td>75</td>\n",
       "      <td>148</td>\n",
       "      <td>0.052798</td>\n",
       "      <td>a</td>\n",
       "      <td>subdued</td>\n",
       "      <td>lovers</td>\n",
       "      <td>that</td>\n",
       "      <td>a</td>\n",
       "      <td>year.”</td>\n",
       "      <td>lovers</td>\n",
       "      <td>that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18547</th>\n",
       "      <td>14</td>\n",
       "      <td>142</td>\n",
       "      <td>0.052798</td>\n",
       "      <td>finding</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>year.”</td>\n",
       "      <td>finding</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>subdued</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36139</th>\n",
       "      <td>40</td>\n",
       "      <td>77</td>\n",
       "      <td>0.052854</td>\n",
       "      <td>I’M</td>\n",
       "      <td>--</td>\n",
       "      <td>to-morrow</td>\n",
       "      <td>,</td>\n",
       "      <td>therefore</td>\n",
       "      <td>--</td>\n",
       "      <td>to-morrow</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       level_0  level_1      dist         f1f          f1t          f2f  \\\n",
       "57231       71      591  0.004978     towards      Bingley      Bingley   \n",
       "22993       19      945  0.007881           I       Office     inspired   \n",
       "22384       19      336  0.007881        have      require           to   \n",
       "50534       57       94  0.007881    inspired        think        think   \n",
       "12506        8     5527  0.008085           ;           we           To   \n",
       "34744       37      333  0.010674          to          his         aunt   \n",
       "52057       61      316  0.011650          ‘I         hope  unforgiving   \n",
       "13360        8     6381  0.015838         the     knocking       letter   \n",
       "60357       75      180  0.016458    strength         here       lovers   \n",
       "60527       76        6  0.018422     general         know       indeed   \n",
       "63380       79     1456  0.018422         you         know     Hunsford   \n",
       "58196       71     1556  0.018422     general         know         know   \n",
       "47312       50     2963  0.020740        have      require         find   \n",
       "11737        8     4758  0.024795     Lydia’s          day       before   \n",
       "68602       86     1865  0.024925         for           us     educated   \n",
       "13603        8     6624  0.024925         the     knocking     educated   \n",
       "63447       79     1523  0.025590        tail  inferiority         like   \n",
       "30963       32      224  0.025935    strength         here        share   \n",
       "12165        8     5186  0.025935  considered        widow     strength   \n",
       "39589       44      834  0.026562          in     disposed         less   \n",
       "20637       16      980  0.026562    educated        grave          age   \n",
       "37131       40     1069  0.027818  considered        widow    to-morrow   \n",
       "39409       44      654  0.027818        with         life   considered   \n",
       "13353        8     6374  0.027818           a    pollution   considered   \n",
       "74864       94      401  0.027818  considered        widow       States   \n",
       "16952       12      947  0.028088    strength         here    gentlemen   \n",
       "14470        8     7491  0.030324      sister            ,           in   \n",
       "13545        8     6566  0.030324        dear         work       sister   \n",
       "27757       26      309  0.032889          it           as           as   \n",
       "11505        8     4526  0.035479       their       defect     concerns   \n",
       "...        ...      ...       ...         ...          ...          ...   \n",
       "68035       86     1298  0.050440       which          the          the   \n",
       "62314       79      390  0.050441           a      subdued       degree   \n",
       "50139       56      465  0.050441        made          not         aunt   \n",
       "63154       79     1230  0.050441          in     disposed       degree   \n",
       "62430       79      506  0.050441         all          too       degree   \n",
       "62625       79      701  0.050441          of      advised       degree   \n",
       "63170       79     1246  0.050441          he         fear       degree   \n",
       "63431       79     1507  0.050441          be    library.”         aunt   \n",
       "28575       28      169  0.050441     rustled          you         aunt   \n",
       "56216       69      753  0.050441          to           be       degree   \n",
       "63069       79     1145  0.050441         his         only         aunt   \n",
       "31559       33      551  0.050441         air           to       degree   \n",
       "56120       69      657  0.050441  ill-humour           be         aunt   \n",
       "7244         8      265  0.050564           ,        man.”         upon   \n",
       "44770       50      421  0.050564         the          way    therefore   \n",
       "4783         6      223  0.050564          in          the         upon   \n",
       "40839       44     2084  0.051088         his         only       family   \n",
       "18277       13      891  0.052798           a       year.”        hoped   \n",
       "74881       94      418  0.052798           a      subdued         cost   \n",
       "69569       87      644  0.052798           a      subdued           of   \n",
       "67000       86      263  0.052798   executors     maternal            a   \n",
       "66482       85      604  0.052798           a      subdued    courtship   \n",
       "74147       93      767  0.052798           ,        after            a   \n",
       "49643       55      949  0.052798           a       year.”            ,   \n",
       "18022       13      636  0.052798           a      subdued      removed   \n",
       "68267       86     1530  0.052798           a       year.”       across   \n",
       "74305       93      925  0.052798          is            a            a   \n",
       "60325       75      148  0.052798           a      subdued       lovers   \n",
       "18547       14      142  0.052798     finding            a            a   \n",
       "36139       40       77  0.052854         I’M           --    to-morrow   \n",
       "\n",
       "             f2t         t1f          t1t          t2f          t2t  \n",
       "57231          ,         Mr.      Bingley      Bingley            ,  \n",
       "22993      think           I       Office           to        think  \n",
       "22384      think        have      require     inspired        think  \n",
       "50534         of          to        think        think           of  \n",
       "12506       must           ,           we           To         must  \n",
       "34744          ,          to          his         aunt         look  \n",
       "52057          ,           I         hope  unforgiving            ,  \n",
       "13360          ;         the     knocking       letter            ,  \n",
       "60357       that    educated        grave       lovers         that  \n",
       "60527       what         you         know       indeed         what  \n",
       "63380         my     general         know     Hunsford           my  \n",
       "58196          ,         you         know         know            ,  \n",
       "47312      chief        have      require     strength         here  \n",
       "11737        had         the          day       before          had  \n",
       "68602      grave         for           us   considered        widow  \n",
       "13603      grave         the     knocking   considered        widow  \n",
       "63447          !        less        taken         like            !  \n",
       "30963         on  considered        widow        share           on  \n",
       "12165       here          of          the           of          the  \n",
       "39589      taken          in     disposed     educated        grave  \n",
       "20637         of        less        taken          age           of  \n",
       "37131          ,        tail  inferiority    to-morrow            ,  \n",
       "39409      widow        with         life         tail  inferiority  \n",
       "13353      widow           a    pollution         tail  inferiority  \n",
       "74864          ;        tail  inferiority       States            ;  \n",
       "16952          ,        less        taken    gentlemen            ,  \n",
       "14470   disposed      sister         that           in     disposed  \n",
       "13545       that        dear         work       sister            ,  \n",
       "27757       soon        soon           as           as           it  \n",
       "11505        Mr.       their        table     concerns          Mr.  \n",
       "...          ...         ...          ...          ...          ...  \n",
       "68035  simpleton       which          the          the      fashion  \n",
       "62314          ?           a      subdued         aunt            ?  \n",
       "50139          ?        made          not       degree            ?  \n",
       "63154          ?          in     disposed         aunt            ?  \n",
       "62430          ?         all          too         aunt            ?  \n",
       "62625          ?          of      advised         aunt            ?  \n",
       "63170          ?          he         fear         aunt            ?  \n",
       "63431          ?          be    library.”       degree            ?  \n",
       "28575          ?     rustled          you       degree            ?  \n",
       "56216          ?          to           be         aunt            ?  \n",
       "63069          ?         his         only       degree            ?  \n",
       "31559          ?         air           to         aunt            ?  \n",
       "56120          ?  ill-humour           be       degree            ?  \n",
       "7244         way           ,        man.”          the          way  \n",
       "44770         --        upon          way    therefore           --  \n",
       "4783         way          in          the          the          way  \n",
       "40839       Lady         his         only       family            ,  \n",
       "18277          .           a      subdued        hoped            .  \n",
       "74881          ;           a       year.”         cost            ;  \n",
       "69569        the           a       year.”           of          the  \n",
       "67000    subdued   executors     maternal            a       year.”  \n",
       "66482          .           a       year.”    courtship            .  \n",
       "74147    subdued           ,        after            a       year.”  \n",
       "49643        and           a      subdued            ,          and  \n",
       "18022          .           a       year.”      removed            .  \n",
       "68267     before           a      subdued       across       before  \n",
       "74305    subdued          is            a            a       year.”  \n",
       "60325       that           a       year.”       lovers         that  \n",
       "18547     year.”     finding            a            a      subdued  \n",
       "36139          ,   therefore           --    to-morrow            ,  \n",
       "\n",
       "[200 rows x 11 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_dup_near.loc[no_dup_near.dist>0].sort_values(by='dist').head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nearests.to_pickle('./nnl2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5b\n",
    "Add a graph on top of the network and begin looking at methods to choose link likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6\n",
    "Combine network and word2vec with bigram transitions. Recognize words as transitions. Especially that. Actually. You might even just try recognizing 3 word transitions like $a \\xrightarrow{b} c$, that's probably the most concrete place to start. Also play around with a high dropout rate. So train batch. Drop / cluster a bunch then continue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7\n",
    "Try implimenting adding in trigrams inbetween bigrams. Basically, train two contexts once you've trained bigrams.\n",
    "Also, implement adding of links for likely things. We are trying to optimize the likelihood, given some word, of the links from that word. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7b\n",
    "This is never going to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8\n",
    "Try to forget the hierarchical stuff. Try seeing how high features can get. Assume a fixed vocabulary size, but words not in the vocab can be added as higher level relations. Actually, try to merge transitions with words. Fixed graph size $|V|^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9a\n",
    "Try to implement some fancy stability constraint stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9b\n",
    "Try to get this system to recognize authors. You can train a classical neural network, but try it at different levels. Try to recognize authors using only the transitions between words. Even if this just performs decent, what does that mean? What interpretation could one draw. Perhaps this is closer to the idealized style recognition that has been hoped for in the past. Also, we can dry to reduce this to the minimum need transitions or maybe try to find singular identifying transitions per author."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10\n",
    "Try to forcast the next sentence from the previous, or the next paragraph from the previous, etc. Try speech generation. Try to untangle mixed up words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
