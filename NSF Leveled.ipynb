{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import math\n",
    "import os.path\n",
    "from six.moves import xrange\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.cluster import Birch\n",
    "import  glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Document Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Doc = collections.namedtuple('Doc',['investigator', 'amount', 'abstract'])\n",
    "\n",
    "def load2(fn):\n",
    "    doc = nltk.data.load(fn)\n",
    "    wrds = nltk.tokenize.wordpunct_tokenize(doc)\n",
    "    investigator = \" \".join(wrds[wrds.index(\"Investigator\")+2:\n",
    "                          min(wrds.index('@' if '@' in wrds else \"Abstract\", \n",
    "                                          wrds.index(\"Investigator\"))-1, \n",
    "                              wrds.index('(', wrds.index(\"Investigator\")))])\n",
    "    amount = int(wrds[wrds.index(\"Amt\") + 4: wrds.index('(', wrds.index(\"Amt\"))][0])\n",
    "    abstract = \" \".join(wrds[wrds.index('Abstract')+2:])\n",
    "    return Doc(investigator, amount, abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Batches Within W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "def gen_batch(data, batch_size, skip_window, num_skips):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # target label at the center of the buffer\n",
    "        targets_to_avoid = [ skip_window ]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target]\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    return batch, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Next Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def complete(KB, data):\n",
    "    data_c = list([data[0]])\n",
    "    data_nl = list()\n",
    "    for index, clust in enumerate(data[:-1]):\n",
    "            i = 0\n",
    "            ed = KB.get_edge_data(clust, data[index+1])\n",
    "            while ed and i < 20:\n",
    "                data_c.append(ed['number'])\n",
    "                data_nl.append(ed['number'])\n",
    "                ed = KB.get_edge_data(ed['number'], data[index+1])\n",
    "                i += 1\n",
    "            data_c.append(data[index+1])\n",
    "    return data_c, data_nl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_level(KB, embeds, data_c, fit):\n",
    "    embed_data = np.array([embeds[i] for i in data_c])\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    nl_raw = [embed_data[0]] + flatten(\n",
    "                np.array([[(sel - embed_data[ind-1]), sel] for ind, sel \n",
    "                       in enumerate(embed_data[1:], start=1)]))\n",
    "    \n",
    "    nl_data = fit.predict(nl_raw)\n",
    "    print(len(nl_data))\n",
    "    for i in range(0, len(nl_data)-1, 2):\n",
    "        KB.add_edge(nl_data[i], nl_data[i+2], number=nl_data[i+1])\n",
    "\n",
    "    return KB, list(nl_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#words = load()\n",
    "#KB = prep_graph(words)\n",
    "batch_size = 256\n",
    "embedding_size = 200  # Dimension of the embedding vector.\n",
    "skip_window = 3      # How many words to consider left and right.\n",
    "num_skips = 4         # How many times to reuse an input to generate a label.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent.\n",
    "valid_size = 10    # Random set of words to evaluate similarity on.\n",
    "valid_window = 200  # Only pick dev samples in the head of the distribution.\n",
    "num_sampled = 100    # Number of negative examples to sample.\n",
    "num_steps = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prep_graph(words):\n",
    "    KB2 = nx.Graph()\n",
    "    for word in words:\n",
    "        if not KB2.has_node(word):\n",
    "            KB2.add_node(word)\n",
    "        \n",
    "    KB3 = nx.convert_node_labels_to_integers(KB2, label_attribute='word')\n",
    "    KB4 = nx.Graph()\n",
    "    \n",
    "    for node in KB3.nodes(True):\n",
    "        KB4.add_node(node[1]['word'], number=node[0])\n",
    "    return KB4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We call this each time we want to add new layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_level(words, embeddings, KB, n_cluster):\n",
    "    for index, word in enumerate(words[:-1]):\n",
    "        if KB.has_node(word) and KB.has_node(words[index+1]):\n",
    "            if KB.has_edge(word, words[index+1]):\n",
    "                node_name = KB.edge[word][words[index+1]]['node']\n",
    "                words = np.insert(words, index+1, str(node_name))\n",
    "    \n",
    "    \n",
    "    data = [KB.node[word]['number'] for word in words if word in KB.node]\n",
    "    embed_data = np.array([embeddings[wordnum] for wordnum in data])\n",
    "    next_lvl_raw = embed_data[1:] - embed_data[:-1]\n",
    "    mbatch = MiniBatchKMeans(n_clusters=n_cluster, \n",
    "                             batch_size=max(len(words)*.05, n_cluster+1), \n",
    "                             max_iter=100000)\n",
    "    next_lvl_cent = mbatch.fit(embed_data)\n",
    "\n",
    "    vocab_size = KB.number_of_nodes()\n",
    "    \n",
    "    for num in range(vocab_size, vocab_size+n_cluster):\n",
    "        KB.add_node(str(num), number=num)\n",
    "    \n",
    "    words_n = np.array([words[0]])\n",
    "    for i in range(1, len(words)-1):\n",
    "        t = next_lvl_cent.labels_[i-1]\n",
    "        words_n = np.append(words_n, [str(t+vocab_size), words[i+1]])\n",
    "        KB.add_edge(words[i], words[i+1], node=str(t+vocab_size))\n",
    "        \n",
    "    return words_n, KB     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now We Test For Maintaining State of W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def W2V2(KB, batch_size, embedding_size, skip_window, num_skips, valid_size,\n",
    "       valid_window, valid_examples, num_sampled, vocabulary_size,\n",
    "       num_steps, filenames, num_files):\n",
    "    graph = tf.Graph()\n",
    "    \n",
    "    with graph.as_default():\n",
    "        def weight_summary(var, name):\n",
    "          \"\"\"Attach a lot of summaries to a Tensor.\"\"\"\n",
    "          with tf.name_scope('summaries'):\n",
    "            mean = tf.reduce_mean(var)\n",
    "            tf.scalar_summary('mean/' + name, mean)\n",
    "            with tf.name_scope('stddev'):\n",
    "                stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "            tf.scalar_summary('stddev/' + name, stddev)\n",
    "            tf.scalar_summary('max/' + name, tf.reduce_max(var))\n",
    "            tf.scalar_summary('min/' + name, tf.reduce_min(var))\n",
    "            tf.histogram_summary(name, var)\n",
    "        \n",
    "                \n",
    "        train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "        train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "        valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "        with tf.device('/cpu:0'):\n",
    "            \n",
    "            embeddings = tf.Variable(\n",
    "                            tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0), name=\"emb\")\n",
    "            weight_summary(embeddings, 'embeddings')\n",
    "            embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "        \n",
    "        nce_weights = tf.Variable(\n",
    "            tf.truncated_normal([vocabulary_size, embedding_size], \n",
    "                               stddev=1.0 / math.sqrt(embedding_size)), name=\"nw\")\n",
    "        weight_summary(nce_weights, 'nce_weights')\n",
    "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]), name=\"nb\")\n",
    "        weight_summary(nce_biases, 'nce_biases')\n",
    "        \n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(nce_weights, nce_biases, embed, train_labels,\n",
    "                          num_sampled, vocabulary_size))\n",
    "        #scalar_summary(loss, 'loss')\n",
    "        optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "        \n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), \n",
    "                                     1, keep_dims=True))\n",
    "        #scalar_summary(norm, 'norm')\n",
    "        \n",
    "        normalized_embeddings = embeddings / norm\n",
    "        weight_summary(normalized_embeddings, 'normalized_embeddings')\n",
    "        valid_embeddings = tf.nn.embedding_lookup(\n",
    "            normalized_embeddings, valid_dataset)\n",
    "        weight_summary(valid_embeddings, 'valid_embeddings')\n",
    "        similarity = tf.matmul(\n",
    "            valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "        weight_summary(similarity, 'similarity')\n",
    "        \n",
    "        merged = tf.merge_all_summaries()\n",
    "        \n",
    "        init = tf.initialize_all_variables()\n",
    "    \n",
    "    \n",
    "    with tf.Session(graph=graph) as session:\n",
    "    \n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        train_writer = tf.train.SummaryWriter('./summaries' + '/train',\n",
    "                                      session.graph)\n",
    "        \n",
    "        init.run()\n",
    "        fit = Birch(threshold=.1, branching_factor=20, n_clusters=None)\n",
    "        if os.path.isfile('./tmp/nips2.ckpt'):\n",
    "            saver.restore(session, './tmp/nips1.ckpt')\n",
    "            print(\"Restored\")\n",
    "        else:\n",
    "            print(\"Initialized\")\n",
    "        \n",
    "        \n",
    "        dq = []\n",
    "        for i in range(num_files):\n",
    "            dq += load2(filenames[i]).abstract.split(\" \")\n",
    "            \n",
    "        dictionary = {}\n",
    "        dat = []\n",
    "        for word in dq:\n",
    "            if word not in dictionary:\n",
    "                dictionary[word] = len(dictionary)\n",
    "            dat += [dictionary[word]]\n",
    "            \n",
    "        \n",
    "        average_loss = 0\n",
    "        for step in xrange(num_steps):\n",
    "            batch_inputs, batch_labels = gen_batch(\n",
    "                dat, batch_size, skip_window, num_skips)\n",
    "            feed_dict = {train_inputs : batch_inputs, \n",
    "                         train_labels : batch_labels}\n",
    "            \n",
    "            if step % 10 == 0:\n",
    "                \n",
    "                _, loss_val, summary = session.run([optimizer, loss, merged], feed_dict=feed_dict)\n",
    "                average_loss += loss_val\n",
    "            \n",
    "                train_writer.add_summary(summary, step)\n",
    "            else:\n",
    "                _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "                average_loss += loss_val\n",
    "        \n",
    "            if step % 2000 == 0:\n",
    "                if step > 0:\n",
    "                    average_loss /= 2000\n",
    "                # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "                print(\"Average loss at step \", step, \": \", average_loss)\n",
    "                average_loss = 0\n",
    "\n",
    "            # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "            if step % 10000 == 0 and step > 0:\n",
    "                sim = similarity.eval()\n",
    "                for i in xrange(valid_size):\n",
    "                    valid_word = revdic[valid_examples[i]]\n",
    "                    top_k = 8 # number of nearest neighbors\n",
    "                    nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                    log_str = \"Nearest to %s:\" % valid_word\n",
    "                    for k in xrange(top_k):\n",
    "                        close_word = revdic[nearest[k]]\n",
    "                        log_str = \"%s %s,\" % (log_str, close_word)\n",
    "                    print(log_str)\n",
    "        saver.save(session, './tmp/nips1.ckpt')\n",
    "        final_embeddings = normalized_embeddings.eval()\n",
    "        return final_embeddings, KB, fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step  0 :  93.8474197388\n",
      "Average loss at step  2000 :  58.8172911773\n",
      "Average loss at step  4000 :  29.4206732301\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-41744a3c85f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m embed = W2V2(KB, batch_size, embedding_size, skip_window, num_skips, \n\u001b[1;32m     22\u001b[0m                    \u001b[0mvalid_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_window\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_sampled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                    vocabulary_size, num_steps, filenames, num_files)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-54b3351b1f9a>\u001b[0m in \u001b[0;36mW2V2\u001b[0;34m(KB, batch_size, embedding_size, skip_window, num_skips, valid_size, valid_window, valid_examples, num_sampled, vocabulary_size, num_steps, filenames, num_files)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mtrain_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                 \u001b[0maverage_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lenny/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lenny/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 915\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    916\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lenny/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 965\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/lenny/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    970\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lenny/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    952\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    953\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "filenames = [fn for fn in glob.iglob('./text/Part*/*/*/*.txt', recursive=False)]\n",
    "num_files = 100\n",
    "vocabulary_size = 10000\n",
    "KB = nx.DiGraph()\n",
    "KB.add_nodes_from(np.arange(10000))\n",
    "embedding_size = 100  # Dimension of the embedding vector.\n",
    "skip_window = 3      # How many words to consider left and right.\n",
    "num_skips = 4         # How many times to reuse an input to generate a label.\n",
    "#batch = batch[:num_skips * (len(batch)//num_skips)]\n",
    "batch_size = 256#len(batch)\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent.\n",
    "valid_size = 5    # Random set of words to evaluate similarity on.\n",
    "valid_window = 10  # Only pick dev samples in the head of the distribution.\n",
    "num_sampled = 20    # Number of negative examples to sample.\n",
    "num_steps = 8000\n",
    "valid_examples = np.random.choice(valid_window, \n",
    "                                      valid_size, replace=False)\n",
    "\n",
    "embed = W2V2(KB, batch_size, embedding_size, skip_window, num_skips, \n",
    "                   valid_size, valid_window, valid_examples, num_sampled,\n",
    "                   vocabulary_size, num_steps, filenames, num_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filenames) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_level2(words, embeddings, KB, n_cluster, fit=None):\n",
    "    words\n",
    "    for index, word in enumerate(words[:-1]):\n",
    "        if KB.has_node(word) and KB.has_node(words[index+1]):\n",
    "            if KB.has_edge(word, words[index+1]):\n",
    "                node_name = KB.edge[word][words[index+1]]['node']\n",
    "                words = np.insert(words, index+1, str(node_name))\n",
    "    \n",
    "    \n",
    "    data = [KB.node[word]['number'] for word in words if word in KB.node]\n",
    "    embed_data = np.array([embeddings[wordnum] for wordnum in data])\n",
    "    next_lvl_raw = embed_data[1:] - embed_data[:-1]\n",
    "    \n",
    "    if not fit:\n",
    "        mbatch = Birch(n_clusters=n_cluster, branching_factor=200)\n",
    "        b_tree = mbatch.fit(embed_data)\n",
    "    else:\n",
    "        fit.set_params(n_clusters=fit.n_clusters+n_cluster)\n",
    "        next_lvl_cent = fit.partial_fit(embed_data)\n",
    "    vocab_size = KB.number_of_nodes()\n",
    "    \n",
    "    for num in range(vocab_size, vocab_size+n_cluster):\n",
    "        KB.add_node(str(num), number=num)\n",
    "    \n",
    "    words_n = np.array([words[0]])\n",
    "    for i in range(1, len(words)-1):\n",
    "        t = next_lvl_cent.labels_[i-1]\n",
    "        words_n = np.append(words_n, [str(t+vocab_size), words[i+1]])\n",
    "        KB.add_edge(words[i], words[i+1], node=str(t+vocab_size))\n",
    "        \n",
    "    return words_n, KB, next_lvl_cent     "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
