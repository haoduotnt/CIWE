{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import math\n",
    "import os.path\n",
    "from six.moves import xrange\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.cluster import Birch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Document Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load():\n",
    "    ppp = nltk.data.load('../../Downloads/ppp.txt', encoding='utf8')\n",
    "    words_p = nltk.tokenize.wordpunct_tokenize(ppp)[130:]\n",
    "    alw = nltk.data.load('../../Downloads/alw.txt', encoding='utf8')\n",
    "    words_a = nltk.tokenize.wordpunct_tokenize(alw)[143:]\n",
    "    return words_a, words_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Batches Within W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "def gen_batch(data, batch_size, skip_window, num_skips):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # target label at the center of the buffer\n",
    "        targets_to_avoid = [ skip_window ]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target]\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    return batch, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(words, vocabulary_size):\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We're Gonna Start By Just Saving Dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def W2V(batch_size, embedding_size, skip_window, num_skips, valid_size,\n",
    "       valid_window, valid_examples, num_sampled, vocabulary_size,\n",
    "       num_steps, data, revdic):\n",
    "    graph = tf.Graph()\n",
    "    \n",
    "    with graph.as_default():\n",
    "        train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "        train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "        valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "        \n",
    "        with tf.device('/cpu:0'):\n",
    "            \n",
    "            embeddings = tf.Variable(\n",
    "                            tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "            embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "        \n",
    "        nce_weights = tf.Variable(\n",
    "            tf.truncated_normal([vocabulary_size, embedding_size], \n",
    "                               stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "        \n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(nce_weights, nce_biases, embed, train_labels,\n",
    "                          num_sampled, vocabulary_size))\n",
    "        optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "        \n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), \n",
    "                                     1, keep_dims=True))\n",
    "        normalized_embeddings = embeddings / norm\n",
    "        valid_embeddings = tf.nn.embedding_lookup(\n",
    "            normalized_embeddings, valid_dataset)\n",
    "        similarity = tf.matmul(\n",
    "            valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "        \n",
    "        init = tf.initialize_all_variables()\n",
    "    \n",
    "    with tf.Session(graph=graph) as session:\n",
    "        \n",
    "        init.run()\n",
    "        print(\"Initialized\")\n",
    "        #saver = tf.train.Saver({'In'})\n",
    "        \n",
    "        average_loss = 0\n",
    "        for step in xrange(num_steps):\n",
    "            batch_inputs, batch_labels = gen_batch(\n",
    "                data, batch_size, skip_window, num_skips)\n",
    "            feed_dict = {train_inputs : batch_inputs, \n",
    "                         train_labels : batch_labels}\n",
    "            \n",
    "            _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "            average_loss += loss_val\n",
    "\n",
    "        \n",
    "            if step % 2000 == 0:\n",
    "                if step > 0:\n",
    "                    average_loss /= 2000\n",
    "                # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "                print(\"Average loss at step \", step, \": \", average_loss)\n",
    "                average_loss = 0\n",
    "\n",
    "            # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "            if step % 10000 == 0 and step > 0:\n",
    "                sim = similarity.eval()\n",
    "                for i in xrange(valid_size):\n",
    "                    valid_word = revdic[valid_examples[i]]\n",
    "                    top_k = 8 # number of nearest neighbors\n",
    "                    nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                    log_str = \"Nearest to %s:\" % valid_word\n",
    "                    for k in xrange(top_k):\n",
    "                        close_word = revdic[nearest[k]]\n",
    "                        log_str = \"%s %s,\" % (log_str, close_word)\n",
    "                    print(log_str)\n",
    "\n",
    "        final_embeddings = normalized_embeddings.eval()\n",
    "        return final_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#words = load()\n",
    "#KB = prep_graph(words)\n",
    "batch_size = 256\n",
    "embedding_size = 200  # Dimension of the embedding vector.\n",
    "skip_window = 3      # How many words to consider left and right.\n",
    "num_skips = 4         # How many times to reuse an input to generate a label.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent.\n",
    "valid_size = 10    # Random set of words to evaluate similarity on.\n",
    "valid_window = 200  # Only pick dev samples in the head of the distribution.\n",
    "num_sampled = 100    # Number of negative examples to sample.\n",
    "num_steps = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classic(l_words, t_words, vocab_size):\n",
    "    data_l, dictionary_l, reverse_dictionary_l = build_vocab(l_words, \n",
    "                                                             vocab_size)\n",
    "    \n",
    "    valid_examples = np.random.choice(valid_window, \n",
    "                                      valid_size, replace=False)\n",
    "    \n",
    "    embeddings_l = W2V(batch_size, embedding_size, skip_window,\n",
    "            num_skips, valid_size, valid_window,\n",
    "            valid_examples, num_sampled, vocab_size,\n",
    "            num_steps, data_l, reverse_dictionary_l)\n",
    "    \n",
    "    data_index = 0\n",
    "    \n",
    "    data_t = [dictionary_l[word_t] \n",
    "              if word_t in dictionary_l else dictionary_l['UNK']\n",
    "              for word_t in t_words]\n",
    "    \n",
    "    \n",
    "    valid_examples = np.random.choice(valid_window, \n",
    "                                      valid_size, replace=False)\n",
    "    \n",
    "    embeddings_t = W2V(batch_size, embedding_size, skip_window,\n",
    "            num_skips, valid_size, valid_window,\n",
    "            valid_examples, num_sampled, vocab_size,\n",
    "            num_steps, data_l, reverse_dictionary_l)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Graph Where Nodes are Words with Number Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prep_graph(words):\n",
    "    KB2 = nx.Graph()\n",
    "    for word in words:\n",
    "        if not KB2.has_node(word):\n",
    "            KB2.add_node(word)\n",
    "        \n",
    "    KB3 = nx.convert_node_labels_to_integers(KB2, label_attribute='word')\n",
    "    KB4 = nx.Graph()\n",
    "    \n",
    "    for node in KB3.nodes(True):\n",
    "        KB4.add_node(node[1]['word'], number=node[0])\n",
    "    return KB4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We call this each time we want to add new layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_level(words, embeddings, KB, n_cluster):\n",
    "    for index, word in enumerate(words[:-1]):\n",
    "        if KB.has_node(word) and KB.has_node(words[index+1]):\n",
    "            if KB.has_edge(word, words[index+1]):\n",
    "                node_name = KB.edge[word][words[index+1]]['node']\n",
    "                words = np.insert(words, index+1, str(node_name))\n",
    "    \n",
    "    \n",
    "    data = [KB.node[word]['number'] for word in words if word in KB.node]\n",
    "    embed_data = np.array([embeddings[wordnum] for wordnum in data])\n",
    "    next_lvl_raw = embed_data[1:] - embed_data[:-1]\n",
    "    mbatch = MiniBatchKMeans(n_clusters=n_cluster, \n",
    "                             batch_size=max(len(words)*.05, n_cluster+1), \n",
    "                             max_iter=100000)\n",
    "    next_lvl_cent = mbatch.fit(embed_data)\n",
    "\n",
    "    vocab_size = KB.number_of_nodes()\n",
    "    \n",
    "    for num in range(vocab_size, vocab_size+n_cluster):\n",
    "        KB.add_node(str(num), number=num)\n",
    "    \n",
    "    words_n = np.array([words[0]])\n",
    "    for i in range(1, len(words)-1):\n",
    "        t = next_lvl_cent.labels_[i-1]\n",
    "        words_n = np.append(words_n, [str(t+vocab_size), words[i+1]])\n",
    "        KB.add_edge(words[i], words[i+1], node=str(t+vocab_size))\n",
    "        \n",
    "    return words_n, KB     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Method\n",
    "We train twice but since we start from scratch each time, it doesn't really matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new(words_l, words_t, vocab_size):\n",
    "    global data_index\n",
    "    data_index = 0\n",
    "    m_common = [item[0] for item in collections.Counter(words_l).most_common(n=500)]\n",
    "    words_l = np.array([word for word in words_l if word in m_common])\n",
    "    KB = prep_graph(words_l)\n",
    "    \n",
    "    valid_examples = np.random.choice(valid_window, \n",
    "                                      valid_size, replace=False)\n",
    "    for i in range(4):\n",
    "        if i > 0:\n",
    "            words_l, KB = add_level(words_l, embeddings_l, KB, 500)\n",
    "        data_index = 0\n",
    "        vocab_size = KB.number_of_nodes()\n",
    "        revdic = {node[1]['number']: node[0] for node in KB.nodes(True)}\n",
    "        data_l = [KB.node[word]['number'] for word in words_l]\n",
    "        embeddings_l = W2V(batch_size, embedding_size, skip_window,\n",
    "                num_skips, valid_size, valid_window, valid_examples, \n",
    "                       num_sampled, vocab_size, 10000, data_l, revdic)\n",
    "    \n",
    "    \n",
    "    words_t = np.array([word for word in words_t if word in m_common])\n",
    "    for index, word in enumerate(words_t[:-1]):\n",
    "        if KB.has_node(word) and KB.has_node(words_t[index+1]):\n",
    "            if KB.has_edge(word, words_t[index+1]):\n",
    "                node_name = KB.edge[word][words_t[index+1]]['node']\n",
    "                words_t = np.insert(words_t, index+1, str(node_name))\n",
    "    \n",
    "    data_index = 0\n",
    "    data_t = [KB.node[word]['number'] for word in words_t if word in KB.node]\n",
    "    embeddings_t = W2V(batch_size, embedding_size, skip_window,\n",
    "                num_skips, valid_size, valid_window, valid_examples, \n",
    "                       num_sampled, vocab_size, num_steps, data_t, revdic)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Test Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_l, words_t = load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step  0 :  234.604553223\n",
      "Average loss at step  2000 :  11.0108101764\n",
      "Average loss at step  4000 :  5.10339176452\n",
      "Average loss at step  6000 :  4.99395320916\n",
      "Average loss at step  8000 :  4.91171428978\n",
      "Average loss at step  10000 :  4.84941917634\n",
      "Nearest to heard: whisper, off, sending, just, Have, CHORUS, Footman, tears,\n",
      "Nearest to get: walked, archbishop, Ugh, LITTLE, *, stoop, ve, but,\n",
      "Nearest to ve: Only, 1, smile, kind, Soup, All, distribution, finished,\n",
      "Nearest to t: *, elbow, haven, washing, milk, tried, hurriedly, don,\n",
      "Nearest to by: none, However, cards, chimney, Duchess, lying, ), shrill,\n",
      "Nearest to electronic: grand, MUST, WILLIAM, fashion, Please, adventures, U, changed,\n",
      "Nearest to !’: ?’, screamed, change, provisions, SAID, feared, Exactly, On,\n",
      "Nearest to things: cautiously, sort, somebody, thimble, prisoner, barrowful, want, PROJECT,\n",
      "Nearest to or: dear, Wake, 11, shaking, crimson, OR, grow, watched,\n",
      "Nearest to UNK: longer, severely, beg, twist, !”, Soo, key, raised,\n",
      "Average loss at step  12000 :  4.80237910736\n",
      "Average loss at step  14000 :  4.77180959308\n",
      "Average loss at step  16000 :  4.74049800622\n",
      "Average loss at step  18000 :  4.70134264922\n",
      "Average loss at step  20000 :  4.67549127531\n",
      "Nearest to heard: whisper, off, sending, Have, just, CHORUS, Footman, tears,\n",
      "Nearest to get: archbishop, Ugh, stoop, walked, LITTLE, remembered, look, moving,\n",
      "Nearest to ve: Only, kind, smile, Soup, 1, d, All, ill,\n",
      "Nearest to t: *, haven, elbow, washing, milk, tried, shared, hurriedly,\n",
      "Nearest to by: none, However, cards, shrill, chimney, lying, receipt, ),\n",
      "Nearest to electronic: grand, MUST, WILLIAM, fashion, Please, U, adventures, work,\n",
      "Nearest to !’: ?’, screamed, provisions, SAID, Exactly, feared, named, On,\n",
      "Nearest to things: cautiously, sort, prisoner, somebody, barrowful, PROJECT, want, thimble,\n",
      "Nearest to or: 11, shaking, OR, grow, license, coaxing, crimson, delighted,\n",
      "Nearest to UNK: beg, longer, remembered, Lory, severely, ordering, twist, !”,\n",
      "Average loss at step  22000 :  4.65044766295\n",
      "Average loss at step  24000 :  4.65274683189\n",
      "Average loss at step  26000 :  4.62286474872\n",
      "Average loss at step  28000 :  4.59847735298\n",
      "Initialized\n",
      "Average loss at step  0 :  234.656295776\n",
      "Average loss at step  2000 :  11.0394166389\n",
      "Average loss at step  4000 :  5.11139304852\n",
      "Average loss at step  6000 :  4.99154132748\n",
      "Average loss at step  8000 :  4.90943279183\n",
      "Average loss at step  10000 :  4.85939238572\n",
      "Nearest to his: garden, C, support, slate, ll, its, the, muchness,\n",
      "Nearest to was: earls, spoke, water, fee, pattering, ?”, others, sister,\n",
      "Nearest to How: youth, offended, grey, evening, re, dead, eat, ME,\n",
      "Nearest to m: WILLIAM, It, speak, hadn, anything, plates, shouted, named,\n",
      "Nearest to think: front, One, want, for, verdict, almost, charge, grinned,\n",
      "Nearest to .: croquet, lasted, shore, tiny, fly, NO, We, state,\n",
      "Nearest to him: led, along, father, Get, IF, As, larger, doubt,\n",
      "Nearest to off: share, caught, s, GUTENBERG, fan, claws, girl, herself,\n",
      "Nearest to Mock: learning, shriek, croquet, row, sneezes, bird, Did, Not,\n",
      "Nearest to say: world, locations, mark, somebody, dishes, journey, play, jumping,\n",
      "Average loss at step  12000 :  4.80599455512\n",
      "Average loss at step  14000 :  4.76279524064\n",
      "Average loss at step  16000 :  4.72995774209\n",
      "Average loss at step  18000 :  4.7098733902\n",
      "Average loss at step  20000 :  4.68719619548\n",
      "Nearest to his: garden, C, support, its, slate, knot, ll, muchness,\n",
      "Nearest to was: earls, spoke, fee, water, sister, ?”, wonder, officers,\n",
      "Nearest to How: grey, youth, offended, evening, re, eat, dead, arms,\n",
      "Nearest to m: WILLIAM, hadn, speak, named, plates, It, legal, thoroughly,\n",
      "Nearest to think: front, One, for, charge, almost, verdict, want, grinned,\n",
      "Nearest to .: croquet, lasted, shore, *, sighing, whiting, funny, remaining,\n",
      "Nearest to him: led, along, father, Get, As, doubt, IF, form,\n",
      "Nearest to off: share, caught, GUTENBERG, fan, footsteps, s, girl, claws,\n",
      "Nearest to Mock: learning, croquet, shriek, sneezes, row, bird, Not, Did,\n",
      "Nearest to say: world, locations, mark, somebody, journey, dishes, jumping, chin,\n",
      "Average loss at step  22000 :  4.65595590794\n",
      "Average loss at step  24000 :  4.63281391764\n",
      "Average loss at step  26000 :  4.61392316139\n",
      "Average loss at step  28000 :  4.61443090844\n"
     ]
    }
   ],
   "source": [
    "classic(words_l, words_t, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step  0 :  153.989837646\n",
      "Average loss at step  2000 :  5.16240739226\n",
      "Average loss at step  4000 :  4.09049435699\n",
      "Average loss at step  6000 :  4.02145858324\n",
      "Average loss at step  8000 :  3.98238668633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1349: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  distances = np.zeros(self.batch_size, dtype=X.dtype)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1360: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  validation_indices = random_state.randint(0, n_samples, init_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:671: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  init_indices = random_state.randint(0, n_samples, init_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1409: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  0, n_samples, self.batch_size)\n",
      "/home/lenny/anaconda3/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1465: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  self.cluster_centers_) for s in slices]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step  0 :  193.846221924\n",
      "Average loss at step  2000 :  5.93933841205\n",
      "Average loss at step  4000 :  3.50263933849\n",
      "Average loss at step  6000 :  3.40718356699\n",
      "Average loss at step  8000 :  3.34587858325\n",
      "Initialized\n",
      "Average loss at step  0 :  222.103393555\n",
      "Average loss at step  2000 :  7.31874467373\n",
      "Average loss at step  4000 :  3.10586724973\n",
      "Average loss at step  6000 :  2.96929880857\n",
      "Average loss at step  8000 :  2.93134997511\n",
      "Initialized\n",
      "Average loss at step  0 :  236.08770752\n",
      "Average loss at step  2000 :  8.83436334783\n",
      "Average loss at step  4000 :  3.03062243599\n",
      "Average loss at step  6000 :  2.81695212293\n",
      "Average loss at step  8000 :  2.74504919231\n",
      "Initialized\n",
      "Average loss at step  0 :  237.448974609\n",
      "Average loss at step  2000 :  8.09999686986\n",
      "Average loss at step  4000 :  3.64540397012\n",
      "Average loss at step  6000 :  3.73201282293\n",
      "Average loss at step  8000 :  3.66827483433\n",
      "Average loss at step  10000 :  3.45715198344\n",
      "Nearest to size: air, as, 503, 662, 723, these, moment, table,\n",
      "Nearest to dear: 544, 862, 805, I, while, 663, 582, nearly,\n",
      "Nearest to her: 953, 527, tone, *, 501, 762, terms, old,\n",
      "Nearest to angrily: ll, 641, 928, 1206, 953, hall, ’, ‘,\n",
      "Nearest to with: 953, 700, At, eyes, for, oh, paragraph, *,\n",
      "Nearest to began: 953, 700, every, States, YOU, is, 1547, 670,\n",
      "Nearest to agreement: 715, *, ., 953, going, business, 501, it,\n",
      "Nearest to Soup: 1878, 1182, 1222, 1170, been, 1335, :, 1315,\n",
      "Nearest to again: *, Then, 527, 741, glad, 558, 727, 780,\n",
      "Nearest to ll: angrily, 1935, there, it, more, ever, 648, 579,\n",
      "Average loss at step  12000 :  3.37008565587\n",
      "Average loss at step  14000 :  3.48041323292\n",
      "Average loss at step  16000 :  3.49720148402\n",
      "Average loss at step  18000 :  3.44317411935\n",
      "Average loss at step  20000 :  3.24092711955\n",
      "Nearest to size: air, 503, as, 662, these, 723, and, here,\n",
      "Nearest to dear: 544, 663, 862, 805, I, while, nearly, well,\n",
      "Nearest to her: 953, ,, 527, *, tone, his, 501, the,\n",
      "Nearest to angrily: 928, ll, 641, 1206, hall, 968, 700, At,\n",
      "Nearest to with: 953, At, for, eyes, paragraph, 700, oh, *,\n",
      "Nearest to began: every, YOU, 700, 953, States, is, 1547, get,\n",
      "Nearest to agreement: 715, ., *, 953, it, going, business, 505,\n",
      "Nearest to Soup: 1878, 1182, 1222, 1170, 1335, been, :, 1315,\n",
      "Nearest to again: Then, *, 953, glad, 741, 727, 504, 527,\n",
      "Nearest to ll: angrily, 1935, there, more, it, ever, 648, 579,\n",
      "Average loss at step  22000 :  3.30220639646\n",
      "Average loss at step  24000 :  3.46966046268\n",
      "Average loss at step  26000 :  3.42783077461\n",
      "Average loss at step  28000 :  3.24050561184\n"
     ]
    }
   ],
   "source": [
    "new(words_l, words_t, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now We Test For Maintaining State of W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def W2V2(batch_size, embedding_size, skip_window, num_skips, valid_size,\n",
    "       valid_window, valid_examples, num_sampled, vocabulary_size,\n",
    "       num_steps, data, revdic):\n",
    "    graph = tf.Graph()\n",
    "    \n",
    "    with graph.as_default():\n",
    "        def weight_summary(var, name):\n",
    "          \"\"\"Attach a lot of summaries to a Tensor.\"\"\"\n",
    "          with tf.name_scope('summaries'):\n",
    "            mean = tf.reduce_mean(var)\n",
    "            tf.scalar_summary('mean/' + name, mean)\n",
    "            with tf.name_scope('stddev'):\n",
    "                stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "            tf.scalar_summary('stddev/' + name, stddev)\n",
    "            tf.scalar_summary('max/' + name, tf.reduce_max(var))\n",
    "            tf.scalar_summary('min/' + name, tf.reduce_min(var))\n",
    "            tf.histogram_summary(name, var)\n",
    "        def scalar_summary(var, name):\n",
    "            with tf.name_scope('summaries'):\n",
    "                tf.scalar_summary('scalar/'+name, var)\n",
    "                \n",
    "        train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "        train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "        valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "        with tf.device('/cpu:0'):\n",
    "            \n",
    "            embeddings = tf.Variable(\n",
    "                            tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0), name=\"emb\")\n",
    "            weight_summary(embeddings, 'embeddings')\n",
    "            embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "        \n",
    "        nce_weights = tf.Variable(\n",
    "            tf.truncated_normal([vocabulary_size, embedding_size], \n",
    "                               stddev=1.0 / math.sqrt(embedding_size)), name=\"nw\")\n",
    "        weight_summary(nce_weights, 'nce_weights')\n",
    "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]), name=\"nb\")\n",
    "        weight_summary(nce_biases, 'nce_biases')\n",
    "        \n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(nce_weights, nce_biases, embed, train_labels,\n",
    "                          num_sampled, vocabulary_size))\n",
    "        #scalar_summary(loss, 'loss')\n",
    "        optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "        \n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), \n",
    "                                     1, keep_dims=True))\n",
    "        #scalar_summary(norm, 'norm')\n",
    "        \n",
    "        normalized_embeddings = embeddings / norm\n",
    "        weight_summary(normalized_embeddings, 'normalized_embeddings')\n",
    "        valid_embeddings = tf.nn.embedding_lookup(\n",
    "            normalized_embeddings, valid_dataset)\n",
    "        weight_summary(valid_embeddings, 'valid_embeddings')\n",
    "        similarity = tf.matmul(\n",
    "            valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "        weight_summary(similarity, 'similarity')\n",
    "        \n",
    "        merged = tf.merge_all_summaries()\n",
    "        \n",
    "        init = tf.initialize_all_variables()\n",
    "    \n",
    "    \n",
    "    with tf.Session(graph=graph) as session:\n",
    "        \n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        train_writer = tf.train.SummaryWriter('./summaries' + '/train',\n",
    "                                      session.graph)\n",
    "        \n",
    "        init.run()\n",
    "        print(\"Initialized\")      \n",
    "        \n",
    "        #saver = tf.train.Saver({'In'})\n",
    "        \n",
    "        average_loss = 0\n",
    "        for step in xrange(num_steps):\n",
    "            batch_inputs, batch_labels = gen_batch(\n",
    "                data, batch_size, skip_window, num_skips)\n",
    "            feed_dict = {train_inputs : batch_inputs, \n",
    "                         train_labels : batch_labels}\n",
    "            \n",
    "            if step % 10 == 0:\n",
    "                \n",
    "                _, loss_val, summary = session.run([optimizer, loss, merged], feed_dict=feed_dict)\n",
    "                average_loss += loss_val\n",
    "            \n",
    "                train_writer.add_summary(summary, step)\n",
    "            else:\n",
    "                _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "                average_loss += loss_val\n",
    "        \n",
    "            if step % 2000 == 0:\n",
    "                if step > 0:\n",
    "                    average_loss /= 2000\n",
    "                # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "                print(\"Average loss at step \", step, \": \", average_loss)\n",
    "                average_loss = 0\n",
    "\n",
    "            # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "            if step % 10000 == 0 and step > 0:\n",
    "                sim = similarity.eval()\n",
    "                for i in xrange(valid_size):\n",
    "                    valid_word = revdic[valid_examples[i]]\n",
    "                    top_k = 8 # number of nearest neighbors\n",
    "                    nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                    log_str = \"Nearest to %s:\" % valid_word\n",
    "                    for k in xrange(top_k):\n",
    "                        close_word = revdic[nearest[k]]\n",
    "                        log_str = \"%s %s,\" % (log_str, close_word)\n",
    "                    print(log_str)\n",
    "        final_embeddings = normalized_embeddings.eval()\n",
    "        return final_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_level2(words, embeddings, KB, n_cluster, fit=None):\n",
    "    words\n",
    "    for index, word in enumerate(words[:-1]):\n",
    "        if KB.has_node(word) and KB.has_node(words[index+1]):\n",
    "            if KB.has_edge(word, words[index+1]):\n",
    "                node_name = KB.edge[word][words[index+1]]['node']\n",
    "                words = np.insert(words, index+1, str(node_name))\n",
    "    \n",
    "    \n",
    "    data = [KB.node[word]['number'] for word in words if word in KB.node]\n",
    "    embed_data = np.array([embeddings[wordnum] for wordnum in data])\n",
    "    next_lvl_raw = embed_data[1:] - embed_data[:-1]\n",
    "    \n",
    "    if not fit:\n",
    "        mbatch = Birch(n_clusters=n_cluster, branching_factor=200)\n",
    "        b_tree = mbatch.fit(embed_data)\n",
    "    else:\n",
    "        fit.set_params(n_clusters=fit.n_clusters+n_cluster)\n",
    "        next_lvl_cent = fit.partial_fit(embed_data)\n",
    "    vocab_size = KB.number_of_nodes()\n",
    "    \n",
    "    for num in range(vocab_size, vocab_size+n_cluster):\n",
    "        KB.add_node(str(num), number=num)\n",
    "    \n",
    "    words_n = np.array([words[0]])\n",
    "    for i in range(1, len(words)-1):\n",
    "        t = next_lvl_cent.labels_[i-1]\n",
    "        words_n = np.append(words_n, [str(t+vocab_size), words[i+1]])\n",
    "        KB.add_edge(words[i], words[i+1], node=str(t+vocab_size))\n",
    "        \n",
    "    return words_n, KB, next_lvl_cent     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic - Maintained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classic(l_words, t_words, vocab_size):\n",
    "    data_l, dictionary_l, reverse_dictionary_l = build_vocab(l_words, \n",
    "                                                             vocab_size)\n",
    "    \n",
    "    valid_examples = np.random.choice(valid_window, \n",
    "                                      valid_size, replace=False)\n",
    "    \n",
    "    embeddings_l = W2V(batch_size, embedding_size, skip_window,\n",
    "            num_skips, valid_size, valid_window,\n",
    "            valid_examples, num_sampled, vocab_size,\n",
    "            num_steps, data_l, reverse_dictionary_l)\n",
    "    \n",
    "    data_index = 0\n",
    "    \n",
    "    data_t = [dictionary_l[word_t] \n",
    "              if word_t in dictionary_l else dictionary_l['UNK']\n",
    "              for word_t in t_words]\n",
    "    \n",
    "    \n",
    "    valid_examples = np.random.choice(valid_window, \n",
    "                                      valid_size, replace=False)\n",
    "    \n",
    "    embeddings_t = W2V(batch_size, embedding_size, skip_window,\n",
    "            num_skips, valid_size, valid_window,\n",
    "            valid_examples, num_sampled, vocab_size,\n",
    "            num_steps, data_l, reverse_dictionary_l)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Method - Maintained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new2(words_l, words_t, vocab_size):\n",
    "    global data_index\n",
    "    data_index = 0\n",
    "    m_common = [item[0] for item in collections.Counter(words_l).most_common(n=500)]\n",
    "    words_l = np.array([word for word in words_l if word in m_common])\n",
    "    KB = prep_graph(words_l)\n",
    "    \n",
    "    valid_examples = np.random.choice(valid_window, \n",
    "                                      valid_size, replace=False)\n",
    "    for i in range(1):\n",
    "        if i > 0:\n",
    "            words_l, KB, fit = add_level2(words_l, embeddings_l, KB, 500)\n",
    "        data_index = 0\n",
    "        #vocab_size = KB.number_of_nodes()\n",
    "        revdic = {node[1]['number']: node[0] for node in KB.nodes(True)}\n",
    "        data_l = [KB.node[word]['number'] for word in words_l]\n",
    "        embeddings_l = W2V2(batch_size, embedding_size, skip_window,\n",
    "                num_skips, valid_size, valid_window, valid_examples, \n",
    "                       num_sampled, vocab_size, 10000, data_l, revdic)\n",
    "    \n",
    "    \n",
    "    words_t = np.array([word for word in words_t if word in m_common])\n",
    "    for index, word in enumerate(words_t[:-1]):\n",
    "        if KB.has_node(word) and KB.has_node(words_t[index+1]):\n",
    "            if KB.has_edge(word, words_t[index+1]):\n",
    "                node_name = KB.edge[word][words_t[index+1]]['node']\n",
    "                words_t = np.insert(words_t, index+1, str(node_name))\n",
    "    \n",
    "    data_index = 0\n",
    "    data_t = [KB.node[word]['number'] for word in words_t if word in KB.node]\n",
    "    embeddings_t = W2V2(batch_size, embedding_size, skip_window,\n",
    "                num_skips, valid_size, valid_window, valid_examples, \n",
    "                       num_sampled, vocab_size, num_steps, data_t, revdic)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Test Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words_l, words_t = load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step  0 :  250.024047852\n",
      "Average loss at step  2000 :  8.58146738505\n",
      "Average loss at step  4000 :  4.14766135931\n",
      "Average loss at step  6000 :  4.02447207868\n",
      "Average loss at step  8000 :  3.94582948947\n"
     ]
    }
   ],
   "source": [
    "new2(words_l, words_t, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scratch"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
