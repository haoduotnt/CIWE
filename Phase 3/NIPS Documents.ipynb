{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import math\n",
    "import os.path\n",
    "from six.moves import xrange\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.cluster import Birch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Document Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load():\n",
    "    ppp = nltk.data.load('../../Downloads/ppp.txt', encoding='utf8')\n",
    "    words_p = nltk.tokenize.wordpunct_tokenize(ppp)[130:]\n",
    "    alw = nltk.data.load('../../Downloads/alw.txt', encoding='utf8')\n",
    "    words_a = nltk.tokenize.wordpunct_tokenize(alw)[143:]\n",
    "    return words_a, words_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Batches Within W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "def gen_batch(data, batch_size, skip_window, num_skips):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    \n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # target label at the center of the buffer\n",
    "        targets_to_avoid = [ skip_window ]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target]\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    return batch, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(words, vocabulary_size):\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We're Gonna Start By Just Saving Dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def W2V2(batch_size, embedding_size, skip_window, num_skips, valid_size,\n",
    "       valid_window, valid_examples, num_sampled, vocabulary_size,\n",
    "       num_steps, data, revdic):\n",
    "    graph = tf.Graph()\n",
    "    \n",
    "    with graph.as_default():\n",
    "        def weight_summary(var, name):\n",
    "          \"\"\"Attach a lot of summaries to a Tensor.\"\"\"\n",
    "          with tf.name_scope('summaries'):\n",
    "            mean = tf.reduce_mean(var)\n",
    "            tf.scalar_summary('mean/' + name, mean)\n",
    "            with tf.name_scope('stddev'):\n",
    "                stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "            tf.scalar_summary('stddev/' + name, stddev)\n",
    "            tf.scalar_summary('max/' + name, tf.reduce_max(var))\n",
    "            tf.scalar_summary('min/' + name, tf.reduce_min(var))\n",
    "            tf.histogram_summary(name, var)\n",
    "        def scalar_summary(var, name):\n",
    "            with tf.name_scope('summaries'):\n",
    "                tf.scalar_summary('scalar/'+name, var)\n",
    "                \n",
    "        train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "        train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "        \n",
    "\n",
    "        with tf.device('/cpu:0'):\n",
    "            \n",
    "            embeddings = tf.Variable(\n",
    "                            tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0), name=\"emb\")\n",
    "            weight_summary(embeddings, 'embeddings')\n",
    "            embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "        \n",
    "        nce_weights = tf.Variable(\n",
    "            tf.truncated_normal([vocabulary_size, embedding_size], \n",
    "                               stddev=1.0 / math.sqrt(embedding_size)), name=\"nw\")\n",
    "        weight_summary(nce_weights, 'nce_weights')\n",
    "        \n",
    "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]), name=\"nb\")\n",
    "        weight_summary(nce_biases, 'nce_biases')\n",
    "        \n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(nce_weights, nce_biases, embed, train_labels,\n",
    "                          num_sampled, vocabulary_size))\n",
    "        #scalar_summary(loss, 'loss')\n",
    "        optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "        \n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), \n",
    "                                     1, keep_dims=True))\n",
    "        #scalar_summary(norm, 'norm')\n",
    "        \n",
    "        normalized_embeddings = embeddings / norm\n",
    "        weight_summary(normalized_embeddings, 'normalized_embeddings')\n",
    "        \n",
    "        valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "        \n",
    "        valid_embeddings = tf.nn.embedding_lookup(\n",
    "            normalized_embeddings, valid_dataset)\n",
    "        weight_summary(valid_embeddings, 'valid_embeddings')\n",
    "        \n",
    "        similarity = tf.matmul(\n",
    "            valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "        weight_summary(similarity, 'similarity')\n",
    "        \n",
    "        merged = tf.merge_all_summaries()\n",
    "        \n",
    "        init = tf.initialize_all_variables()\n",
    "    \n",
    "    \n",
    "    with tf.Session(graph=graph) as session:\n",
    "        \n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        train_writer = tf.train.SummaryWriter('./summaries' + '/train',\n",
    "                                      session.graph)\n",
    "        \n",
    "        init.run()\n",
    "        print(\"Initialized\")      \n",
    "        \n",
    "        #saver = tf.train.Saver({'In'})\n",
    "        \n",
    "        average_loss = 0\n",
    "        for step in xrange(num_steps):\n",
    "            batch_inputs, batch_labels = gen_batch(\n",
    "                data, batch_size, skip_window, num_skips)\n",
    "            feed_dict = {train_inputs : batch_inputs, \n",
    "                         train_labels : batch_labels}\n",
    "            \n",
    "            if step % 10 == 0:\n",
    "                \n",
    "                _, loss_val, summary = session.run([optimizer, loss, merged], feed_dict=feed_dict)\n",
    "                average_loss += loss_val\n",
    "            \n",
    "                train_writer.add_summary(summary, step)\n",
    "            else:\n",
    "                _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "                average_loss += loss_val\n",
    "        \n",
    "            if step % 2000 == 0:\n",
    "                if step > 0:\n",
    "                    average_loss /= 2000\n",
    "                # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "                print(\"Average loss at step \", step, \": \", average_loss)\n",
    "                average_loss = 0\n",
    "\n",
    "            # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "            if step % 10000 == 0 and step > 0:\n",
    "                sim = similarity.eval()\n",
    "                for i in xrange(valid_size):\n",
    "                    valid_word = revdic[valid_examples[i]]\n",
    "                    top_k = 8 # number of nearest neighbors\n",
    "                    nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                    log_str = \"Nearest to %s:\" % valid_word\n",
    "                    for k in xrange(top_k):\n",
    "                        close_word = revdic[nearest[k]]\n",
    "                        log_str = \"%s %s,\" % (log_str, close_word)\n",
    "                    print(log_str)\n",
    "        final_embeddings = normalized_embeddings.eval()\n",
    "        return final_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#words = load()\n",
    "#KB = prep_graph(words)\n",
    "batch_size = 256\n",
    "embedding_size = 200  # Dimension of the embedding vector.\n",
    "skip_window = 3      # How many words to consider left and right.\n",
    "num_skips = 4         # How many times to reuse an input to generate a label.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent.\n",
    "valid_size = 10    # Random set of words to evaluate similarity on.\n",
    "valid_window = 200  # Only pick dev samples in the head of the distribution.\n",
    "num_sampled = 100    # Number of negative examples to sample.\n",
    "num_steps = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Graph Where Nodes are Words with Number Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prep_graph(words):\n",
    "    KB2 = nx.Graph()\n",
    "    for word in words:\n",
    "        if not KB2.has_node(word):\n",
    "            KB2.add_node(word)\n",
    "        \n",
    "    KB3 = nx.convert_node_labels_to_integers(KB2, label_attribute='word')\n",
    "    KB4 = nx.Graph()\n",
    "    \n",
    "    for node in KB3.nodes(True):\n",
    "        KB4.add_node(node[1]['word'], number=node[0])\n",
    "    return KB4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We call this each time we want to add new layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_level(words, embeddings, KB, n_cluster):\n",
    "    for index, word in enumerate(words[:-1]):\n",
    "        if KB.has_node(word) and KB.has_node(words[index+1]):\n",
    "            if KB.has_edge(word, words[index+1]):\n",
    "                node_name = KB.edge[word][words[index+1]]['node']\n",
    "                words = np.insert(words, index+1, str(node_name))\n",
    "    \n",
    "    \n",
    "    data = [KB.node[word]['number'] for word in words if word in KB.node]\n",
    "    embed_data = np.array([embeddings[wordnum] for wordnum in data])\n",
    "    next_lvl_raw = embed_data[1:] - embed_data[:-1]\n",
    "    mbatch = MiniBatchKMeans(n_clusters=n_cluster, \n",
    "                             batch_size=max(len(words)*.05, n_cluster+1), \n",
    "                             max_iter=100000)\n",
    "    next_lvl_cent = mbatch.fit(embed_data)\n",
    "\n",
    "    vocab_size = KB.number_of_nodes()\n",
    "    \n",
    "    for num in range(vocab_size, vocab_size+n_cluster):\n",
    "        KB.add_node(str(num), number=num)\n",
    "    \n",
    "    words_n = np.array([words[0]])\n",
    "    for i in range(1, len(words)-1):\n",
    "        t = next_lvl_cent.labels_[i-1]\n",
    "        words_n = np.append(words_n, [str(t+vocab_size), words[i+1]])\n",
    "        KB.add_edge(words[i], words[i+1], node=str(t+vocab_size))\n",
    "        \n",
    "    return words_n, KB     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_level2(words, embeddings, KB, fit=None):\n",
    "    words\n",
    "    for index, word in enumerate(words[:-1]):\n",
    "        if KB.has_node(word) and KB.has_node(words[index+1]):\n",
    "            if KB.has_edge(word, words[index+1]):\n",
    "                node_name = KB.edge[word][words[index+1]]['node']\n",
    "                words = np.insert(words, index+1, str(node_name))\n",
    "    \n",
    "    \n",
    "    data = [KB.node[word]['number'] for word in words if word in KB.node]\n",
    "    embed_data = np.array([embeddings[wordnum] for wordnum in data])\n",
    "    next_lvl_raw = embed_data[1:] - embed_data[:-1]\n",
    "    \n",
    "    if not fit:\n",
    "        mbatch = Birch(n_clusters=None, branching_factor=100)\n",
    "        b_tree = mbatch.fit(embed_data)\n",
    "    else:\n",
    "        fit.set_params(n_clusters=fit.n_clusters+n_cluster)\n",
    "        next_lvl_cent = fit.partial_fit(embed_data)\n",
    "    vocab_size = KB.number_of_nodes()\n",
    "    \n",
    "    for num in range(vocab_size, vocab_size+n_cluster):\n",
    "        KB.add_node(str(num), number=num)\n",
    "    \n",
    "    words_n = np.array([words[0]])\n",
    "    for i in range(1, len(words)-1):\n",
    "        t = next_lvl_cent.labels_[i-1]\n",
    "        words_n = np.append(words_n, [str(t+vocab_size), words[i+1]])\n",
    "        KB.add_edge(words[i], words[i+1], node=str(t+vocab_size))\n",
    "        \n",
    "    return words_n, KB, next_lvl_cent     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Method - Maintained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new2(words_l, words_t, vocab_size):\n",
    "    global data_index\n",
    "    data_index = 0\n",
    "    m_common = [item[0] for item in collections.Counter(words_l).most_common(n=500)]\n",
    "    words_l = np.array([word for word in words_l if word in m_common])\n",
    "    KB = prep_graph(words_l)\n",
    "    \n",
    "    valid_examples = np.random.choice(valid_window, \n",
    "                                      valid_size, replace=False)\n",
    "    for i in range(1):\n",
    "        if i > 0:\n",
    "            words_l, KB, fit = add_level2(words_l, embeddings_l, KB, 500)\n",
    "        data_index = 0\n",
    "        #vocab_size = KB.number_of_nodes()\n",
    "        revdic = {node[1]['number']: node[0] for node in KB.nodes(True)}\n",
    "        data_l = [KB.node[word]['number'] for word in words_l]\n",
    "        embeddings_l = W2V2(batch_size, embedding_size, skip_window,\n",
    "                num_skips, valid_size, valid_window, valid_examples, \n",
    "                       num_sampled, vocab_size, 10000, data_l, revdic)\n",
    "    \n",
    "    \n",
    "    words_t = np.array([word for word in words_t if word in m_common])\n",
    "    for index, word in enumerate(words_t[:-1]):\n",
    "        if KB.has_node(word) and KB.has_node(words_t[index+1]):\n",
    "            if KB.has_edge(word, words_t[index+1]):\n",
    "                node_name = KB.edge[word][words_t[index+1]]['node']\n",
    "                words_t = np.insert(words_t, index+1, str(node_name))\n",
    "    \n",
    "    data_index = 0\n",
    "    data_t = [KB.node[word]['number'] for word in words_t if word in KB.node]\n",
    "    embeddings_t = W2V2(batch_size, embedding_size, skip_window,\n",
    "                num_skips, valid_size, valid_window, valid_examples, \n",
    "                       num_sampled, vocab_size, num_steps, data_t, revdic)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Test Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words_l, words_t = load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step  0 :  161.569137573\n",
      "Average loss at step  2000 :  5.26270027351\n",
      "Average loss at step  4000 :  4.2121324532\n",
      "Average loss at step  6000 :  4.1391200037\n",
      "Average loss at step  8000 :  4.10277059472\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 500\n",
    "m_common = [item[0] for item in collections.Counter(words_l).most_common(n=500)]\n",
    "words_l = np.array([word for word in words_l if word in m_common])\n",
    "KB = prep_graph(words_l)\n",
    "revdic = {node[1]['number']: node[0] for node in KB.nodes(True)}\n",
    "data_l = [KB.node[word]['number'] for word in words_l]\n",
    "valid_examples = np.random.choice(valid_window, \n",
    "                                      valid_size, replace=False)\n",
    "embeddings_l = W2V2(batch_size, embedding_size, skip_window,\n",
    "                num_skips, valid_size, valid_window, valid_examples, \n",
    "                       num_sampled, vocab_size, 10000, data_l, revdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = [KB.node[word]['number'] for word in words_l if word in KB.node]\n",
    "embed_data = np.array([embeddings_l[wordnum] for wordnum in data])\n",
    "next_lvl_raw = embed_data[1:] - embed_data[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fit = Birch(n_clusters=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(40, 60):\n",
    "    fit.partial_fit(next_lvl_raw[i*500:(i+1)*500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Birch(branching_factor=50, compute_labels=True, copy=True, n_clusters=500,\n",
       "   threshold=0.5)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit.set_params(n_clusters=500)\n",
    "fit.partial_fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100,  67, 113, ...,  49, 394, 256])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit.predict(next_lvl_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fit2 = MiniBatchKMeans(n_clusters=500, \n",
    "                             batch_size=max(len(next_lvl_raw)*.05, 500+1), \n",
    "                             max_iter=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
